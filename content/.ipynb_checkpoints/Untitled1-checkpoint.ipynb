{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following the Kaggle Tutorial\n",
    "\n",
    "We work with a dataset containing 25,000 movie reviews from IMDB, labeled by sentiment (positive/negative).  In addition, there are another 50,000 IMDB reviews provided without any rating labels.  \n",
    "\n",
    "The reviews are split evenly into train and test sets (25k train and 25k test). The overall distribution of labels is also balanced within the train set (12.5k pos and 12.5k neg).  Our goal is to see how well we can predict sentiment in the test dataset. \n",
    "\n",
    "We note that Keras also has a version of this dataset, but it is missing the unlabeled reviews.  We could download the original dataset [here](http://ai.stanford.edu/~amaas/data/sentiment/), but we settle on a slighly more formatted (and so easier-to-use) version from Kaggle [here](https://www.kaggle.com/c/word2vec-nlp-tutorial/data?unlabeledTrainData.tsv.zip).\n",
    "\n",
    "For this first part of the notebook, we follow along with the Kaggle tutorial [here](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexis/anaconda/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # for importing the data\n",
    "from bs4 import BeautifulSoup # for removing HTML tags\n",
    "import re # for processing text with regular expressions\n",
    "import nltk.data # for sentence splitting\n",
    "import logging # output messages for word2vec embedding step\n",
    "from gensim.models import word2vec # embedding algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "train = pd.read_csv(\"data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv( \"data/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv(\"data/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `train` is a pandas dataframe with 25,000 rows, each corresponding to a review.  There are three columns:\n",
    "- `id`: an identifier\n",
    "- `sentiment` $\\in \\{0,1\\}$: indicates pos(1) or neg(0) sentiment\n",
    "- `review`: the full text of the review\n",
    "\n",
    "`test` and `unlabeled_train` have 25,000 and 50,000 rows, respectively, and each contain two columns (`id` and `review`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 test reviews, and 50000 unlabeled train reviews.\n",
      "\n",
      "TOTAL: 100000 reviews.\n",
      "\n",
      "In the labeled train set, 12500 reviews are positive, and 12500 are negative.\n"
     ]
    }
   ],
   "source": [
    "print(\"Read %d labeled train reviews, %d test reviews, \" \\\n",
    " \"and %d unlabeled train reviews.\\n\" % (train[\"review\"].size,  \n",
    " test[\"review\"].size, unlabeled_train[\"review\"].size))\n",
    "print(\"TOTAL: %d reviews.\\n\" % int(train[\"review\"].size + test[\"review\"].size + unlabeled_train[\"review\"].size))\n",
    "print(\"In the labeled train set, %s reviews are positive, and %s are negative.\" % (sum(train[\"sentiment\"].values), train.shape[0]-sum(train[\"sentiment\"].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize a sample review below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like ¨Jurassik Park¨, and some scientists resurrect one of nature\\'s most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons. The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. The giant animals savagely are stalking its prey and the group run afoul and fight against one nature\\'s most fearsome predators. Furthermore a third Sabretooth more dangerous and slow stalks its victims.<br /><br />The movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the Sabretooths appear with mediocre special effects.The story provides exciting and stirring entertainment but it results to be quite boring .The giant animals are majority made by computer generator and seem totally lousy .Middling performances though the players reacting appropriately to becoming food.Actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls . And it packs a ridiculous final deadly scene. No for small kids by realistic,gory and violent attack scenes . Other films about Sabretooths or Smilodon are the following : ¨Sabretooth(2002)¨by James R Hickox with Vanessa Angel, David Keith and John Rhys Davies and the much better ¨10.000 BC(2006)¨ by Roland Emmerich with with Steven Strait, Cliff Curtis and Camilla Belle. This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films. Miller is an Australian director usually working for television (Tidal wave, Journey to the center of the earth, and many others) and occasionally for cinema ( The man from Snowy river, Zeus and Roxanne,Robinson Crusoe ). Rating : Below average, bottom of barrel.\"'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"review\"][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to process each review to make it mathematically digestible.  We focus on using distributed word vectors created by the Word2Vec algorithm.  We use the implementation of Word2Vec from the gensim package.\n",
    "\n",
    "Word2Vec takes as input single sentences, each one as a list of words.  That is, the input format is a list of lists.  It outputs a model of semantic meaning. \n",
    "\n",
    "We split a paragraph into sentences through the use of NLTK's __punkt__ tokenizer.  As an example, we print the first sentence identified by __punkt__ from the review above.  It nicely corresponds to our expectations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park .\n"
     ]
    }
   ],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Visualize first stripped sentence\n",
    "print(tokenizer.tokenize(train['review'][2].strip())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would furthermore need each sentence to be cleaned.  There are HTML tags such as `<br/>`, abbreviations, punctuation - all common issues when processing text from online. We need to tidy up the text before applying machine learning algorithms.\n",
    "\n",
    "Our function `raw_sentence_to_wordlist` works as follows:\n",
    "- HTML tags are removed through the use of the Beautiful Soup library.\n",
    "- Numbers and punctuation are replaced with a space using regular expressions.\n",
    "- We convert all of the words to lowercase and convert the sentence string into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def raw_sentence_to_wordlist( raw_sentence ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_sentence, \"lxml\").get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters (replace numbers and punctuation, for instance, with a space)      \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    wordlist = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. Return a list of words.\n",
    "    return(wordlist)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we output the cleaned version of the sentence above.  Again, it appears to be working nicely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'film', 'starts', 'with', 'a', 'manager', 'nicholas', 'bell', 'giving', 'welcome', 'investors', 'robert', 'carradine', 'to', 'primal', 'park']\n"
     ]
    }
   ],
   "source": [
    "print(raw_sentence_to_wordlist(tokenizer.tokenize(train['review'][2].strip())[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost done with processing the input for the Word2Vec embedding.  Now, we need only create a function (here, `review_to_sentences`) that puts everything together by looping through the paragraph and creates lists of lists of clean sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( raw_sentence_to_wordlist( raw_sentence ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need only to call this function to populate our list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexis/anaconda/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/Users/alexis/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexis/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/alexis/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/alexis/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/alexis/anaconda/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/Users/alexis/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/alexis/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "# I chose not to use the tqdm package here, but it is a possibility for tracking longer runtimes.\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Word2Vec algorithm accepts a number of parameters that must be trained.  Here, we provide some starting points (that were trained by the writers of the Kaggle tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: Configure logging module so that Word2Vec creates nice output messages\n",
    "# I chose not to use it, because it runs quite fast on my system.\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model \n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and save the model for later use. \n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)\n",
    "# to load: model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `most_similar` function to get insight into the model's word clusters.  This function returns the words that are closest to the provided word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.675002932548523),\n",
       " ('horrible', 0.6517869234085083),\n",
       " ('good', 0.642318069934845),\n",
       " ('lousy', 0.6187937259674072),\n",
       " ('awful', 0.5989417433738708),\n",
       " ('crappy', 0.5930088758468628),\n",
       " ('stupid', 0.581573486328125),\n",
       " ('cheesy', 0.5734360218048096),\n",
       " ('lame', 0.5684062242507935),\n",
       " ('atrocious', 0.5386618375778198)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we have a reasonably good model for semantic meaning :).  In case you're wondering how the model is stored, we can look beneath the hood.  The Word2Vec model consists of a feature vector for each word in the vocabulary, stored in a numpy array called \"syn0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(16490, 300)\n"
     ]
    }
   ],
   "source": [
    "print(type(model.syn0))\n",
    "print(model.syn0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual word vectors can be accessed in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[ 0.01623794  0.00502289  0.09394965  0.03428585 -0.00123815]\n"
     ]
    }
   ],
   "source": [
    "# Each word is assinged a 1x300 numpy array\n",
    "print(model[\"bad\"].shape)\n",
    "\n",
    "# As an example, we output the first 5 entries of the array assigned to \"bad\" \n",
    "print(model[\"bad\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a representation of each document, with the $i$-th document as a $w_i$x300-dimensional matrix, where $w_i$ is the number of words in the $i$-th document.  Unlike the bag-of-words representation, this embedding captures the dynamics of word order and represents a useful improvement that could be fed into a potential classification algorithm.\n",
    "\n",
    "However, there is a problem.  Classification and clustering algorithms typically require the text input to be represented as a fixed length vector.  The Kaggle tutorial investigates two methods to deal with this issue.  The resultant test accuracy was then compared to the results of simply running a Random Forest classifer on bag-of-words features.\n",
    "\n",
    "The first method was to simply average the word vectors in a given review; they also removed stop words in processing.  The resultant set of average word vectors was then fed into a Random Forest classifier.  This method (slightly) underperformed bag-of-words.\n",
    "\n",
    "The second method was to cluster the set of words, according to their Word2Vec embeddings.  They used K-means for this purpose, with the number of clusters set to one fifth of the vocabulary size ($K$ = 16490/5 = 3298).  (We note that while some clusters made intuitive sense, others did not.)  Documents were then represented as \"bags-of-centroids\", where each document was represented as a K=3298-dimensional vector, where the $i$-th entry contained the number of words in the document in the $i$-th cluster.  The resultant bag-of-centroids vectors were then fed into a Random Forest classifier.  This method (slightly) underperformed bag of words.\n",
    "\n",
    "Thus, we must find a more clever approach to using our Word2Vec features. \n",
    "\n",
    "# Deviating from Kaggle Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
