{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at two approaches to face detection that both first find a lower-dimensional subspace onto which to project an image of a face.\n",
    "\n",
    "Recall that **covariance** is a measure of how two random variables change together.  It is a scalar value whose magnitude is not easily interpretable.  We say that two random variables are **uncorrelated** if and only if their covariance is zero. \n",
    "\n",
    "pictures of uncorrelated random variables, idea of choosing new basis functions ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and Eigenfaces\n",
    "\n",
    "PCA follows the following idea: given a dataset $X$ containing $n$ data points, each with $m$ features, we would like to find a new set of $m'$ (where $m' < m$) features that still captures the same relevant patterns in the data, albeit with less noise.\n",
    "\n",
    "Suppose our dataset is contained in an $m \\times n $ matrix $X$, whose $j$-th column $x_{\\cdot j} \\in \\mathbb{R}^m$ is the $j$-th datapoint.  Note that the column space is at most an $m$-dimensional object, anyway, and so we need only $m$ vectors to span it.\n",
    "\n",
    "Let $P$ be an $m \\times m$ matrix, and let $p_i$ be the $i$-th column.  Suppose that the $\\{p_i\\}_{i=1}^m$ are a set of orthogonal vectors that span the same space as our dataset.  Then we can express each data point in terms of the new basis and write that for all $j \\in \\{1, \\ldots, n\\}$,\n",
    "\n",
    "$$\n",
    "x_{\\cdot j} = \\sum_{i=1}^m (p_i \\cdot x_{\\cdot j}) p_i.\n",
    "$$\n",
    "\n",
    "We can keep track of the coefficients in a matrix $Y = P^TX$, whose $i,j$-th entry $Y_{ij}$ is $p_i \\cdot x_{\\cdot j}$.  \n",
    "\n",
    "We are interested in choosing some $m'<m$ so that\n",
    "$$\n",
    "x_{\\cdot j} \\approx \\sum_{i=1}^{m'} (p_i \\cdot x_{\\cdot j}) p_i,\n",
    "$$\n",
    "\n",
    "where noise is gone but pattern still there.  How do we choose the entries of our matrix $P$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA and Fisherfaces\n",
    "\n",
    "LDA follows the same principle as PCA, but makes use of the labels that accompany each datapoint.  (PCA ignored them and in some sense can be thought of as \"unsupervised\"; LDA takes a \"supervised\" approach.)  The main idea is that we would like to find a projection where data points with the same label are close together, and separated from points with a different label. **linear discriminant analysis not to be confused with latent dirichlet allocation!**. characterizes points (still keep patterns) but also separates data points belonging to different classes.\n",
    "\n",
    "Towards that end, we define two metrics: one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
