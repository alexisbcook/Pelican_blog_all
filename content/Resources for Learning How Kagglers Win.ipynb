{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, I did not prepare a blog post on ML topics.  Instead, I've been working towards a solution towards the [\"Bosch Production Line Performance\"](https://www.kaggle.com/c/bosch-production-line-performance) Kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first step was to figure out how to load (into Python) the data, which was huge (>1 million data points, each with >1000 features).  I tried many approaches and initially thought that loading the csv file into an SFrame was the [best approach](https://turi.com/products/create/docs/generated/graphlab.SFrame.read_csv.html#graphlab.SFrame.read_csv), as the time to load was quite fast; however, later processing turned out to be slow (for instance, extracting and processing a small subset of the SFrame took 10+ min).  When I found out that I could extract csv data from zip files without unzipping and storing a huge dataset on my computer, I was excited and hopeful.  However, the [zipfile](https://docs.python.org/3/library/zipfile.html) module turned out to be too slow for my purposes; as was working with HDF5 files through the [h5py](https://www.getdatajoy.com/learn/Read_and_Write_HDF5_from_Python) module.  The [read_csv](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) command in pandas ended up being the best option (especially when the time involved in processing, exploration, and inference was taken into account)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptualizing the Model(s)\n",
    "\n",
    "Many Kaggle competitions are won through an ensembling approach.  Thus, I've been reading more about this technique that is used by many data scientists:\n",
    "\n",
    "- [Kaggle Ensembling Guide](http://mlwave.com/kaggle-ensembling-guide/)\n",
    "    - contains a link to this [accessible paper](http://arxiv.org/pdf/0911.0460.pdf)\n",
    "    \n",
    "- [Talk on Ensembling and Gradient Boosted Trees](http://nycdatascience.com/featured-talk-1-kaggle-data-scientist-owen-zhang/)\n",
    "\n",
    "I have also been reading about gradient boosted trees:\n",
    "\n",
    "- [Introduction to XGBoost](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)\n",
    "- [Very Simple XGBoost Example with Data](https://www.kaggle.com/cbrogan/titanic/xgboost-example-python/run/1620)\n",
    "- [Another XGBoost Example with Data](https://jessesw.com/XG-Boost/)\n",
    "- [XGBoost Documentation](http://xgboost.readthedocs.io/en/latest/)\n",
    "- [XGBoost Parameters](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md)\n",
    "- [Guide to Parameter Tuning in XGBoost](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n",
    "\n",
    "An even more accessible introduction to boosted trees can be found through the Coursera course below (see weeks 3-5):\n",
    "- [Machine Learning: Classification](https://www.coursera.org/learn/ml-classification)\n",
    "\n",
    "For now, I thought I would share my sources and what I've been up to; in future weeks, I hope to have more to discuss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
