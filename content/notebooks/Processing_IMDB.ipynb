{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We work with a dataset containing 50,000 movie reviews from IMDB, labeled by sentiment (positive/negative).  In addition, there are another 50,000 IMDB reviews provided without any rating labels.  \n",
    "\n",
    "The reviews are split evenly into train and test sets (25k train and 25k test). The overall distribution of labels is also balanced within the train and test sets (12.5k pos and 12.5k neg).  Our goal is to predict sentiment in the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexis/anaconda/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os                                # accessing directory of files\n",
    "import pandas as pd                      # storing the data\n",
    "from bs4 import BeautifulSoup            # removing HTML tags\n",
    "import re                                # text processing with regular expressions\n",
    "from gensim.models import word2vec       # embedding algorithm\n",
    "import numpy as np                       # arrays and other mathy structures     \n",
    "from tqdm import tqdm                    # timing algorithms\n",
    "from gensim import models                # doc2vec implementation\n",
    "from random import shuffle               # for shuffling reviews\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import Sequential      # deep learning (part 1)\n",
    "from keras.layers import Dense, Dropout  # deep learning (part 2)\n",
    "%matplotlib inline                       \n",
    "\n",
    "# If you are using Python 3, you will get an error.\n",
    "# (Pattern is a Python 2 library and fails to install for Python 3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be downloaded [here](http://ai.stanford.edu/~amaas/data/sentiment/).  We first write code to extract the reviews into Pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 12500 pos train reviews \n",
      " 12500 neg train reviews \n",
      " 12500 pos test reviews \n",
      " 12500 neg test reviews \n",
      " 50000 unsup reviews\n",
      "\n",
      " TOTAL: 100000 reviews\n"
     ]
    }
   ],
   "source": [
    "def load_data(directory_name):\n",
    "    # load dataset from directory to Pandas dataframe\n",
    "    data = []\n",
    "    files = [f for f in os.listdir('../../../aclImdb/' + directory_name)]\n",
    "    for f in files:\n",
    "        with open('../../../aclImdb/' + directory_name + f, \"r\", encoding = 'utf-8') as myfile:\n",
    "            data.append(myfile.read())\n",
    "    df = pd.DataFrame({'review': data, 'file': files})\n",
    "    return df\n",
    "\n",
    "# load training dataset\n",
    "train_pos = load_data('train/pos/')\n",
    "train_neg = load_data('train/neg/')\n",
    "\n",
    "# load test dataset\n",
    "test_pos = load_data('test/pos/')\n",
    "test_neg = load_data('test/neg/')\n",
    "\n",
    "# load unsupervised dataset\n",
    "unsup = load_data('train/unsup/')\n",
    "\n",
    "print(\"\\n %d pos train reviews \\n %d neg train reviews \\n %d pos test reviews \\n %d neg test reviews \\n %d unsup reviews\" \\\n",
    "      % (train_pos.shape[0], train_neg.shape[0], test_pos.shape[0], test_neg.shape[0], unsup.shape[0]))\n",
    "print(\"\\n TOTAL: %d reviews\" % int(train_pos.shape[0] + train_neg.shape[0] + test_pos.shape[0] + test_neg.shape[0] + unsup.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_pos`, `train_neg`, `test_pos`, `test_neg`, and `unsup` are Pandas dataframes.  They each have two columns, and each row corresponds to a review:\n",
    "- `file` : name of file that contains review\n",
    "- `review` : the full text of the review\n",
    "\n",
    "We write a function `review_to_wordlist`, which processes each review as follows:\n",
    "- Punctuation is made consistent through the use of regular expressions.\n",
    "- HTML tags are removed through the use of the Beautiful Soup library.\n",
    "- All words are converted to lowercase.\n",
    "- Each review is converted into a list of words.\n",
    "\n",
    "We note that there is still some room for improvement.  For instance, \n",
    "- Strings like \"Sgt. Cutter\" currently are broken into two sentences.  We should instead determine how to differentiate between periods that signify the end of an abbreviation and periods that denote the end of a sentence.\n",
    "- Some writers separate their sentences with commas or line breaks; the algorithm currently absorbs these multiple sentences into an individual sentence.\n",
    "- Ellipses (...) are currently processed as multiple, empty sentences (which are then discarded).\n",
    "\n",
    "Before writing this post, I read the Kaggle tutorial [here](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors).  My current processing algorithm borrows from that page, but also adds some meaningful improvements, partially informed by the algorithm [here](https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py).  For instance, \n",
    "- We keep the punctuation that ends each sentence (i.e., period vs. exclamation point), whereas punctuation was discarded in the Kaggle tutorial.  \n",
    "- We do smarter processing of contractions, in a way that understands that \"should've\" = \"should\" + \"'ve\".  In the Kaggle tutorial, \"should've\" is kept as a single word (contractions are not understood in terms of their composite parts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_str( string ):\n",
    "    # Function that cleans text using regular expressions\n",
    "    string = re.sub(r' +', ' ', string)\n",
    "    string = re.sub(r'\\.+', '.', string)\n",
    "    string = re.sub(r'\\.(?! )', '. ', string)    \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'m\", \" \\'m\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" ( \", string) \n",
    "    string = re.sub(r\"\\)\", \" ) \", string) \n",
    "    string = re.sub(r\"\\?\", \" ? \", string) \n",
    "    string = re.sub(r\"\\.\", \" . \", string)\n",
    "    string = re.sub(r\"\\-\", \" - \", string)\n",
    "    string = re.sub(r\"\\;\", \" ; \", string)\n",
    "    string = re.sub(r\"\\:\", \" : \", string)\n",
    "    string = re.sub(r'\\\"', ' \" ', string)\n",
    "    string = re.sub(r'\\/', ' / ', string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each cleaned review is fed into the `LabeledLineReview` class, written below.  These labeled reviews are fed into the Doc2Vec algorithm to obtain an embedding of each review.  We note that we use the full set of 100,000 reviews to learn the embedding, but our classification algorithm will be trained with the training set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Doc2VecUtility(object):\n",
    "\n",
    "    def review_to_wordlist( review ):\n",
    "        #\n",
    "        # Function to turn each review into a list of sentences, \n",
    "        # where each sentence is a list of words\n",
    "        #\n",
    "        # 1. Process punctuation, excessive periods, missing spaces \n",
    "        review = clean_str(review)\n",
    "        #\n",
    "        # 2. Remove HTML tags \n",
    "        review = BeautifulSoup(review, \"lxml\").get_text()\n",
    "        #\n",
    "        # 3. remove white spaces\n",
    "        review = review.strip()\n",
    "        #\n",
    "        # 4. return lowercase collection of words\n",
    "        wordlist = review.lower().split()\n",
    "        #\n",
    "        # Return the list of words\n",
    "        return wordlist\n",
    "\n",
    "    class LabeledLineReview(object):\n",
    "        def __init__(self, dflist):\n",
    "            self.dflist = dflist\n",
    "\n",
    "        def __iter__(self):\n",
    "            for df in self.dflist:\n",
    "                for idx in tqdm(df.index):\n",
    "                    yield models.doc2vec.LabeledSentence(review_to_wordlist(df.ix[idx, 'review']), [df.ix[idx, 'file']])\n",
    "\n",
    "        def to_array(self):\n",
    "            self.reviews = []\n",
    "            for df in self.dflist:\n",
    "                for idx in tqdm(df.index):\n",
    "                    self.reviews.append(models.doc2vec.LabeledSentence(review_to_wordlist(df.ix[idx, 'review']), [df.ix[idx, 'file']]))\n",
    "            return self.reviews\n",
    "\n",
    "        def reviews_perm(self):\n",
    "            shuffle(self.reviews)\n",
    "            return self.reviews\n",
    "    \n",
    "    def train(self):\n",
    "        # train the model and save it to a file\n",
    "        reviews = LabeledLineReview([train_pos, train_neg, test_pos, test_neg, unsup])\n",
    "             \n",
    "        num_features = 100    # Word vector dimensionality                      \n",
    "        min_word_count = 1   # Minimum word count                        \n",
    "        num_workers = 8       # Number of threads to run in parallel\n",
    "        context = 10          # Context window size                                                                                    \n",
    "        downsampling = 1e-4   # Downsample setting for frequent words\n",
    "\n",
    "        model = models.Doc2Vec(workers = num_workers, \\\n",
    "                               size = num_features, min_count = min_word_count, \\\n",
    "                               window = context, sample = downsampling, negative = 5)\n",
    "        model.build_vocab(reviews.to_array())\n",
    "\n",
    "        for epoch in tqdm(range(10)):\n",
    "            model.train(reviews.reviews_perm())\n",
    "\n",
    "        model.init_sims(replace=True)\n",
    "        model.save(\"models/imdb_100\")\n",
    "                \n",
    "    def get_embedding(self):\n",
    "        # if the model is not already saved, train the model\n",
    "        if not os.path.isfile('models/imdb_100'):\n",
    "            self.train()\n",
    "        model = models.Doc2Vec.load(\"models/imdb_100\")\n",
    "        \n",
    "        # obtain train data embeddings and labels\n",
    "        train_array = np.zeros((25000, 100))\n",
    "        train_tags = list(train_pos['file'].values) + list(train_neg['file'].values)\n",
    "        for idx , val in enumerate(train_tags):\n",
    "            train_array[idx] = model.docvecs[val]\n",
    "        train_labels = np.append(np.ones(12500), np.zeros(12500))\n",
    "\n",
    "        # obtain test data embeddings and labels\n",
    "        test_array = np.zeros((25000, 100))\n",
    "        test_tags = list(test_pos['file'].values) + list(test_neg['file'].values)\n",
    "        for idx , val in enumerate(test_tags):\n",
    "            test_array[idx] = model.docvecs[val]\n",
    "        test_labels = np.append(np.ones(12500), np.zeros(12500))\n",
    "        \n",
    "        return train_array, train_labels, test_array, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[d2v_train, train_labels, d2v_test, test_labels] = Doc2VecUtility().get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8992"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(d2v_train, train_labels)\n",
    "classifier.score(d2v_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo!  We can predict sentiment with nearly 90 percent accuracy!  Can we do better?  Let's try out a MLP with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2909 - acc: 0.8825     \n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2640 - acc: 0.8916     \n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2583 - acc: 0.8935     \n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2534 - acc: 0.8970     \n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2476 - acc: 0.8995     \n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2446 - acc: 0.9028     \n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2394 - acc: 0.9044     \n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2356 - acc: 0.9082     \n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2319 - acc: 0.9099     \n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2278 - acc: 0.9141     \n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2229 - acc: 0.9135     \n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2201 - acc: 0.9171     \n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2161 - acc: 0.9194     \n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2116 - acc: 0.9195     \n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2107 - acc: 0.9221     \n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2069 - acc: 0.9237     \n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2044 - acc: 0.9258     \n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2007 - acc: 0.9266     \n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.1968 - acc: 0.9271     \n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.1981 - acc: 0.9276     \n",
      "\n",
      " 0.90468\n"
     ]
    }
   ],
   "source": [
    "# Now, we try a multilayer perceptron (with one hidden layer) in Keras !\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Dense(200, input_dim=100, init='uniform', activation='relu'))\n",
    "keras_model.add(Dropout(0.5))\n",
    "keras_model.add(Dense(1, activation='sigmoid'))\n",
    "keras_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "keras_model.fit(d2v_train, train_labels, nb_epoch=20, batch_size=20)\n",
    "loss, accuracy = keras_model.evaluate(d2v_test, test_labels, verbose=0)\n",
    "print(\"\\n\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, it's a slight improvement, but we haven't done much better.  To improve, our intuition tells us we should try some combination of:\n",
    "- better string cleaning,\n",
    "- testing other parameters for Doc2Vec embedding step, and\n",
    "- constructing deeper neural networks.\n",
    "\n",
    "However, it seems that Kaggle competitiors were not able to make this work -- that is, we won't be able to do much better with the current plan.  Thus, we will try something else, informed by the techniques [here](https://www.kaggle.com/c/word2vec-nlp-tutorial/forums/t/14966/post-competition-solutions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review_to_lists_of_lists( review, tokenizer ):\n",
    "    # \n",
    "    # Function to turn each review into a list of sentences, \n",
    "    # where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Process punctuation, excessive periods, missing spaces \n",
    "    review = clean_str(review)\n",
    "    #\n",
    "    # 2. Remove HTML tags \n",
    "    review = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    #\n",
    "    # 3. Use the NLTK tokenizer to split the review into list of sentences\n",
    "    #   (getting rid of extra spaces at front/back)\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 4. Loop over each sentence to get list of list of lowercase words\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # Convert to lowercase and split into list of words\n",
    "        raw_sentence = raw_sentence.lower().split()\n",
    "        # If a sentence is not long enough, skip it\n",
    "        if len(raw_sentence) > 1:\n",
    "            # add list of words to returned object\n",
    "            sentences.append( raw_sentence )\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences\n",
    "\n",
    "def corpus_to_list(df, tokenizer):\n",
    "    # Turns dataframe of reviews into a list of sentences,\n",
    "    # where each sentence is a list of words\n",
    "    # and sentences are derived from *all reviews* in dataframe df\n",
    "    sentences = []\n",
    "    for idx in tqdm(df.index):\n",
    "        to_append = review_to_lists_of_lists(df.ix[idx, 'review'], tokenizer)\n",
    "        sentences += to_append\n",
    "    return sentences\n",
    "\n",
    "train_pos_sentences = corpus_to_list(train_pos, tokenizer)\n",
    "train_neg_sentences = corpus_to_list(train_neg, tokenizer)\n",
    "test_pos_sentences = corpus_to_list(test_pos, tokenizer)\n",
    "test_neg_sentences = corpus_to_list(test_neg, tokenizer)\n",
    "unsup_sentences = corpus_to_list(unsup, tokenizer)\n",
    "\n",
    "sentences = train_pos_sentences + train_neg_sentences + test_pos_sentences + test_neg_sentences + unsup_sentences\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model \n",
    "model = word2vec.Word2Vec(sentences, workers = num_workers, \\\n",
    "            size = num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
