{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We work with a dataset containing 50,000 movie reviews from IMDB, labeled by sentiment (positive/negative).  In addition, there are another 50,000 IMDB reviews provided without any rating labels.  \n",
    "\n",
    "The reviews are split evenly into train and test sets (25k train and 25k test). The overall distribution of labels is also balanced within the train and test sets (12.5k pos and 12.5k neg).  Our goal is to predict sentiment in the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexis/anaconda/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import os                                # accessing directory of files\n",
    "import pandas as pd                      # storing the data\n",
    "from bs4 import BeautifulSoup            # removing HTML tags\n",
    "import re                                # text processing with regular expressions\n",
    "from gensim.models import word2vec       # embedding algorithm\n",
    "import numpy as np                       # arrays and other mathy structures     \n",
    "from tqdm import tqdm                    # timing algorithms\n",
    "from gensim import models                # doc2vec implementation\n",
    "from random import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%matplotlib inline                       \n",
    "\n",
    "# If you are using Python 3, you will get an error.\n",
    "# (Pattern is a Python 2 library and fails to install for Python 3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be downloaded [here](http://ai.stanford.edu/~amaas/data/sentiment/).  We first write code to extract the reviews into Pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 12500 pos train reviews \n",
      " 12500 neg train reviews \n",
      " 12500 pos test reviews     \n",
      " 12500 neg test reviews \n",
      " 50000 unsup reviews\n",
      "\n",
      " TOTAL: 100000 reviews\n"
     ]
    }
   ],
   "source": [
    "def load_data(directory_name):\n",
    "    # load dataset from directory to Pandas dataframe\n",
    "    data = []\n",
    "    files = [f for f in os.listdir('../../aclImdb/' + directory_name)]\n",
    "    for f in files:\n",
    "        with open('../../aclImdb/' + directory_name + f, \"r\", encoding = 'utf-8') as myfile:\n",
    "            data.append(myfile.read())\n",
    "    df = pd.DataFrame({'review': data, 'file': files})\n",
    "    return df\n",
    "\n",
    "# load training dataset\n",
    "train_pos = load_data('train/pos/')\n",
    "train_neg = load_data('train/neg/')\n",
    "\n",
    "# load test dataset\n",
    "test_pos = load_data('test/pos/')\n",
    "test_neg = load_data('test/neg/')\n",
    "\n",
    "# load unsupervised dataset\n",
    "unsup = load_data('train/unsup/')\n",
    "\n",
    "print(\"\\n %d pos train reviews \\n %d neg train reviews \\n %d pos test reviews \\n %d neg test reviews \\n %d unsup reviews\" \\\n",
    "      % (train_pos.shape[0], train_neg.shape[0], test_pos.shape[0], test_neg.shape[0], unsup.shape[0]))\n",
    "print(\"\\n TOTAL: %d reviews\" % int(train_pos.shape[0] + train_neg.shape[0] + test_pos.shape[0] + test_neg.shape[0] + unsup.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_pos`, `train_neg`, `test_pos`, `test_neg`, and `unsup` are Pandas dataframes.  They each have two columns, and each row corresponds to a review:\n",
    "- `file` : name of file that contains review\n",
    "- `review` : the full text of the review\n",
    "\n",
    "We write a function `review_to_wordlist`, which processes each review as follows:\n",
    "- Punctuation is made consistent through the use of regular expressions.\n",
    "- HTML tags are removed through the use of the Beautiful Soup library.\n",
    "- All words are converted to lowercase.\n",
    "- Each review is converted into a list of words.\n",
    "\n",
    "We note that there is still some room for improvement.  For instance, \n",
    "- Strings like \"Sgt. Cutter\" currently are broken into two sentences.  We should instead determine how to differentiate between periods that signify the end of an abbreviation and periods that denote the end of a sentence.\n",
    "- Some writers separate their sentences with commas or line breaks; the algorithm currently absorbs these multiple sentences into an individual sentence.\n",
    "- Ellipses (...) are currently processed as multiple, empty sentences (which are then discarded).\n",
    "\n",
    "Before writing this post, I read the Kaggle tutorial [here](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors).  My current processing algorithm borrows from that page, but also adds some meaningful improvements, partially informed by the algorithm [here](https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py).  For instance, \n",
    "- We keep the punctuation that ends each sentence (i.e., period vs. exclamation point), whereas punctuation was discarded in the Kaggle tutorial.  \n",
    "- We do smarter processing of contractions, in a way that understands that \"should've\" = \"should\" + \"'ve\".  In the Kaggle tutorial, \"should've\" is kept as a single word (contractions are not understood in terms of their composite parts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_str( string ):\n",
    "    # Function that cleans text using regular expressions\n",
    "    string = re.sub(r' +', ' ', string)\n",
    "    string = re.sub(r'\\.+', '.', string)\n",
    "    string = re.sub(r'\\.(?! )', '. ', string)    \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'m\", \" \\'m\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" ( \", string) \n",
    "    string = re.sub(r\"\\)\", \" ) \", string) \n",
    "    string = re.sub(r\"\\?\", \" ? \", string) \n",
    "    string = re.sub(r\"\\.\", \" . \", string)\n",
    "    string = re.sub(r\"\\-\", \" - \", string)\n",
    "    string = re.sub(r\"\\;\", \" ; \", string)\n",
    "    string = re.sub(r\"\\:\", \" : \", string)\n",
    "    string = re.sub(r'\\\"', ' \" ', string)\n",
    "    string = re.sub(r'\\/', ' / ', string)\n",
    "    return string\n",
    "\n",
    "def review_to_wordlist( review ):\n",
    "    #\n",
    "    # Function to turn each review into a list of sentences, \n",
    "    # where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Process punctuation, excessive periods, missing spaces \n",
    "    review = clean_str(review)\n",
    "    #\n",
    "    # 2. Remove HTML tags \n",
    "    review = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    #\n",
    "    # 3. remove white spaces\n",
    "    review = review.strip()\n",
    "    #\n",
    "    # 4. return lowercase collection of words\n",
    "    wordlist = review.lower().split()\n",
    "    #\n",
    "    # Return the list of words\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each cleaned review is fed into the `LabeledLineReview` class, written below.  These labeled reviews are fed into the Doc2Vec algorithm to obtain an embedding of each review.  We note that we use the full set of 100,000 reviews to learn the embedding, but our classification algorithm (here, we use Linear Regression) will be trained with the training set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LabeledLineReview(object):\n",
    "    def __init__(self, dflist):\n",
    "        self.dflist = dflist\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for df in self.dflist:\n",
    "            for idx in tqdm(df.index):\n",
    "                yield models.doc2vec.LabeledSentence(review_to_wordlist(df.ix[idx, 'review']), [df.ix[idx, 'file']])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.reviews = []\n",
    "        for df in self.dflist:\n",
    "            for idx in tqdm(df.index):\n",
    "                self.reviews.append(models.doc2vec.LabeledSentence(review_to_wordlist(df.ix[idx, 'review']), [df.ix[idx, 'file']]))\n",
    "        return self.reviews\n",
    "    \n",
    "    def reviews_perm(self):\n",
    "        shuffle(self.reviews)\n",
    "        return self.reviews\n",
    "    \n",
    "reviews = LabeledLineReview([train_pos, train_neg, test_pos, test_neg, unsup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:07<00:00, 1620.67it/s]\n",
      "100%|██████████| 12500/12500 [00:07<00:00, 1592.76it/s]\n",
      "100%|██████████| 12500/12500 [00:07<00:00, 1626.76it/s]\n",
      "100%|██████████| 12500/12500 [00:07<00:00, 1645.64it/s]\n",
      "100%|██████████| 50000/50000 [00:28<00:00, 1761.05it/s]\n",
      "100%|██████████| 10/10 [20:12<00:00, 120.20s/it]\n"
     ]
    }
   ],
   "source": [
    "model = models.Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)\n",
    "model.build_vocab(reviews.to_array())\n",
    "\n",
    "for epoch in tqdm(range(10)):\n",
    "    model.train(reviews.reviews_perm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# obtain train data embeddings and labels\n",
    "train_array = np.zeros((25000, 100))\n",
    "train_tags = list(train_pos['file'].values) + list(train_neg['file'].values)\n",
    "for idx , val in enumerate(train_tags):\n",
    "    train_array[idx] = model.docvecs[val]\n",
    "train_labels = np.append(np.ones(12500), np.zeros(12500))\n",
    "\n",
    "# obtain test data embeddings and labels\n",
    "test_array = np.zeros((25000, 100))\n",
    "test_tags = list(test_pos['file'].values) + list(test_neg['file'].values)\n",
    "for idx , val in enumerate(test_tags):\n",
    "    test_array[idx] = model.docvecs[val]\n",
    "test_labels = np.append(np.ones(12500), np.zeros(12500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89927999999999997"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_array, train_labels)\n",
    "classifier.score(test_array, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo!  We can predict sentiment with nearly 90 percent accuracy!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
