{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following the Kaggle Tutorial\n",
    "\n",
    "We work with a dataset containing 25,000 movie reviews from IMDB, labeled by sentiment (positive/negative).  In addition, there are another 50,000 IMDB reviews provided without any rating labels.  \n",
    "\n",
    "The reviews are split evenly into train and test sets (25k train and 25k test). The overall distribution of labels is also balanced within the train set (12.5k pos and 12.5k neg).  Our goal is to see how well we can predict sentiment in the test dataset. \n",
    "\n",
    "We note that Keras also has a version of this dataset, but it is missing the unlabeled reviews.  We could download the original dataset [here](http://ai.stanford.edu/~amaas/data/sentiment/), but we settle on a slighly more formatted (and so easier-to-use) version from Kaggle [here](https://www.kaggle.com/c/word2vec-nlp-tutorial/data?unlabeledTrainData.tsv.zip).\n",
    "\n",
    "For this first part of the notebook, we follow along with the Kaggle tutorial [here](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd                 # for importing the data\n",
    "from bs4 import BeautifulSoup       # for removing HTML tags\n",
    "import re                           # for processing with regular expressions\n",
    "import nltk.data                    # for sentence splitting\n",
    "import logging                      # output messages for word2vec embedding step\n",
    "from gensim.models import word2vec  # embedding algorithm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# If you are using Python 3, you will get an error.\n",
    "# (Pattern is a Python 2 library and fails to install for Python 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "train = pd.read_csv(\"../data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"../data/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv(\"../data/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `train` is a pandas dataframe with 25,000 rows, each corresponding to a review.  There are three columns:\n",
    "- `id`: an identifier\n",
    "- `sentiment` $\\in \\{0,1\\}$: indicates pos(1) or neg(0) sentiment\n",
    "- `review`: the full text of the review\n",
    "\n",
    "`test` and `unlabeled_train` have 25,000 and 50,000 rows, respectively, and each contain two columns (`id` and `review`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 test reviews, and 50000 unlabeled train reviews.\n",
      "\n",
      "TOTAL: 100000 reviews.\n",
      "\n",
      "In the labeled train set, 12500 reviews are positive, and 12500 are negative.\n"
     ]
    }
   ],
   "source": [
    "print(\"Read %d labeled train reviews, %d test reviews, \" \\\n",
    " \"and %d unlabeled train reviews.\\n\" % (train[\"review\"].size,  \n",
    " test[\"review\"].size, unlabeled_train[\"review\"].size))\n",
    "print(\"TOTAL: %d reviews.\\n\" % int(train[\"review\"].size + test[\"review\"].size + unlabeled_train[\"review\"].size))\n",
    "print(\"In the labeled train set, %s reviews are positive, and %s are negative.\" % (sum(train[\"sentiment\"].values), train.shape[0]-sum(train[\"sentiment\"].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize a sample review below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like ¨Jurassik Park¨, and some scientists resurrect one of nature\\'s most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons. The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. The giant animals savagely are stalking its prey and the group run afoul and fight against one nature\\'s most fearsome predators. Furthermore a third Sabretooth more dangerous and slow stalks its victims.<br /><br />The movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the Sabretooths appear with mediocre special effects.The story provides exciting and stirring entertainment but it results to be quite boring .The giant animals are majority made by computer generator and seem totally lousy .Middling performances though the players reacting appropriately to becoming food.Actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls . And it packs a ridiculous final deadly scene. No for small kids by realistic,gory and violent attack scenes . Other films about Sabretooths or Smilodon are the following : ¨Sabretooth(2002)¨by James R Hickox with Vanessa Angel, David Keith and John Rhys Davies and the much better ¨10.000 BC(2006)¨ by Roland Emmerich with with Steven Strait, Cliff Curtis and Camilla Belle. This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films. Miller is an Australian director usually working for television (Tidal wave, Journey to the center of the earth, and many others) and occasionally for cinema ( The man from Snowy river, Zeus and Roxanne,Robinson Crusoe ). Rating : Below average, bottom of barrel.\"'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"review\"][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to process each review to make it mathematically digestible.  We focus on using distributed word vectors created by the Word2Vec algorithm.  We use the implementation of Word2Vec from the gensim package.\n",
    "\n",
    "Word2Vec takes as input single sentences, each one as a list of words.  That is, the input format is a list of lists.  It outputs a model of semantic meaning. \n",
    "\n",
    "We split a paragraph into sentences through the use of NLTK's __punkt__ tokenizer.  As an example, we print the first sentence identified by __punkt__ from the review above.  It nicely corresponds to our expectations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park .\n"
     ]
    }
   ],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Convert sample review to list of sentences\n",
    "stripped_sentences_sample = tokenizer.tokenize(train['review'][2].strip())\n",
    "# Visualize first sample sentence\n",
    "print(stripped_sentences_sample[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would furthermore need each sentence to be cleaned.  There are HTML tags such as `<br/>`, abbreviations, punctuation - all common issues when processing text from online. We need to tidy up the text before applying machine learning algorithms.\n",
    "\n",
    "Our function `raw_sentence_to_wordlist` works as follows:\n",
    "- HTML tags are removed through the use of the Beautiful Soup library.\n",
    "- Numbers and punctuation are replaced with a space using regular expressions.\n",
    "- We convert all of the words to lowercase and convert the sentence string into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def raw_sentence_to_wordlist( raw_sentence ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_sentence, \"lxml\").get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters (replace numbers and punctuation, for instance, with a space)      \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    wordlist = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. Return a list of words.\n",
    "    return(wordlist)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we output the cleaned version of the sentence above.  Again, it appears to be working nicely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'film', 'starts', 'with', 'a', 'manager', 'nicholas', 'bell', 'giving', 'welcome', 'investors', 'robert', 'carradine', 'to', 'primal', 'park']\n"
     ]
    }
   ],
   "source": [
    "print(raw_sentence_to_wordlist(stripped_sentences_sample[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost done with processing the input for the Word2Vec embedding.  Now, we need only create a function (here, `review_to_sentences`) that puts everything together by looping through the paragraph and creates lists of lists of clean sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( raw_sentence_to_wordlist( raw_sentence ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need only to call this function to populate our list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I chose not to use the tqdm package here, but it is a possibility for tracking longer runtimes.\n",
    "train_sentences = []  # Initialize an empty list of sentences\n",
    "unlabeled_sentences = []\n",
    "labels = []\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for index, row in train.iterrows():\n",
    "    to_append = review_to_sentences(train['review'][index], tokenizer)\n",
    "    train_sentences += to_append\n",
    "    labels += list(train['sentiment'][index]*np.ones(len(to_append)))\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for index, row in unlabeled_train.iterrows():\n",
    "    to_append = review_to_sentences(unlabeled_train['review'][index], tokenizer)\n",
    "    unlabeled_sentences += to_append\n",
    "\n",
    "# concatenate the two lists\n",
    "sentences = train_sentences + unlabeled_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Word2Vec algorithm accepts a number of parameters that must be trained.  Here, we provide some starting points (that were supplied by the writers of the Kaggle tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: Configure logging module so that Word2Vec creates nice output messages\n",
    "# I chose not to use it, because it runs quite fast on my system.\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model \n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and save the model for later use. \n",
    "# model.save(\"300features_40minwords_10context\")\n",
    "# to load: model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `most_similar` function to get insight into the model's word clusters.  This function returns the words that are closest to the provided word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.6697828769683838),\n",
       " ('horrible', 0.646095871925354),\n",
       " ('good', 0.6342213153839111),\n",
       " ('lousy', 0.6001288890838623),\n",
       " ('awful', 0.5992096662521362),\n",
       " ('crappy', 0.5987610816955566),\n",
       " ('stupid', 0.5869919061660767),\n",
       " ('cheesy', 0.5790674686431885),\n",
       " ('lame', 0.5578579306602478),\n",
       " ('dumb', 0.5522592067718506)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we have a reasonably good model for semantic meaning :).  In case you're wondering how the model is stored, we can look beneath the hood.  The Word2Vec model consists of a feature vector for each word in the vocabulary, stored in a numpy array called \"syn0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(16490, 300)\n"
     ]
    }
   ],
   "source": [
    "print(type(model.syn0))\n",
    "print(model.syn0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual word vectors can be accessed in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[ 0.04584237 -0.10387777 -0.07895309  0.07299399  0.061274  ]\n"
     ]
    }
   ],
   "source": [
    "# Each word is assinged a 1x300 numpy array\n",
    "print(model[\"bad\"].shape)\n",
    "\n",
    "# As an example, we output the first 5 entries of the array assigned to \"bad\" \n",
    "print(model[\"bad\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a representation of each document, with the $i$-th document as a matrix with 300 columns and $w_i$ rows, where $w_i$ is the number of words in the $i$-th document.  Unlike the bag-of-words representation, this embedding captures the dynamics of word order and represents a useful improvement that could be fed into a potential classification algorithm.\n",
    "\n",
    "However, we soon run into a problem.  Classification and clustering algorithms typically require the text input to be represented as a fixed length vector.  The Kaggle tutorial investigates two methods to deal with this issue.  The resultant test accuracy was then compared to the results of simply running a Random Forest classifer on bag-of-words features.\n",
    "\n",
    "The first method was to simply average the word vectors in a given review; they also removed stop words in processing.  The resultant set of average word vectors was then fed into a Random Forest classifier.  This method (slightly) underperformed bag-of-words.\n",
    "\n",
    "The second method was to cluster the set of words, according to their Word2Vec embeddings.  They used $K$-means for this purpose, with the number of clusters set to one fifth of the vocabulary size ($K$ = 16490/5 = 3298).  Documents were then represented as \"bags-of-centroids\", where each document was represented as a $K$-dimensional vector, where the $i$-th entry contained the number of words in the document in the $i$-th cluster.  The resultant bag-of-centroids vectors were then fed into a Random Forest classifier.  This method (slightly) underperformed bag of words.\n",
    "\n",
    "Thus, we must find a more clever approach towards using our Word2Vec features. \n",
    "\n",
    "# Deviating from Kaggle Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sentence has 969 words.\n",
      "The shortest sentence has 0 words.\n",
      "We have a total of 2 classes.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# obtain shortest and longest sentences\n",
    "(max_sentence_id, longest_sentence) = max(enumerate(sentences), key = lambda sentences: len(sentences[1]))\n",
    "(min_sentence_id, shortest_sentence) = min(enumerate(sentences), key = lambda sentences: len(sentences[1]))\n",
    "\n",
    "print(\"The longest sentence has %s words.\" % len(longest_sentence))\n",
    "print(\"The shortest sentence has %s words.\" % len(shortest_sentence))\n",
    "\n",
    "num_classes = len(train['sentiment'].unique())\n",
    "print(\"We have a total of %s classes.\" % num_classes)\n",
    "print(shortest_sentence)\n",
    "\n",
    "embedding_size = 100\n",
    "vocab_size = 1000\n",
    "sequence_length = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 1591076\n",
      "Total number of labeled sentences: 266551\n",
      "Total number of unlabeled sentences: 528987\n",
      "795538\n",
      "Sentences of length 0: 8936\n",
      "Sentences of length >200: 890 ( <=200: 1590186 )\n",
      "\n",
      "Sentences of length >100: 10808 ( <=100: 1580268 )\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAGICAYAAABcEoPrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ba020080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain list of sentence lengths\n",
    "sentence_lengths = [len(elem) for elem in sentences]\n",
    "sentence_lengths.sort()\n",
    "\n",
    "# get total number of sentences\n",
    "print(\"Total number of sentences:\", len(sentence_lengths))\n",
    "print(\"Total number of labeled sentences:\", len(train_sentences))\n",
    "print(\"Total number of unlabeled sentences:\", len(unlabeled_sentences))\n",
    "print(len(train_sentences+unlabeled_sentences))\n",
    "# get number of sentences of length zero\n",
    "print(\"Sentences of length 0:\", sum([i==0 for i in sentence_lengths]))\n",
    "# get number of sentences of size greater than 200\n",
    "print(\"Sentences of length >200:\", sum([i>200 for i in sentence_lengths]), \n",
    "      \"( <=200:\", sum([i<=200 for i in sentence_lengths]), \")\\n\")\n",
    "# get number of sentences of size greater than 100\n",
    "print(\"Sentences of length >100:\", sum([i>100 for i in sentence_lengths]), \n",
    "      \"( <=100:\", sum([i<=100 for i in sentence_lengths]), \")\\n\")\n",
    "\n",
    "# make plots\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# make histogram of sentence lengths for sentences <=200 \n",
    "plt.subplot(121)\n",
    "plt.hist(sentence_lengths, bins=np.linspace(0, 200, 21))\n",
    "plt.title(\"Histogram of sentence lengths up to 200\")\n",
    "\n",
    "# make histogram of sentence lengths for sentences <=100 \n",
    "plt.subplot(122)\n",
    "plt.hist(sentence_lengths, bins=np.linspace(0, 100, 11))\n",
    "plt.title(\"Histogram of sentence lengths up to 100\")\n",
    "\n",
    "# adjust spacing between subplots to minimize the overlaps\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, model, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "        \n",
    "        # save the model\n",
    "        self.model = model\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            # set embeddings\n",
    "            W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\", trainable=False)\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-38c5ef4999f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         cnn = TextCNN(\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        sess.run(cnn.W.assign(cnn.model.syn0))\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
