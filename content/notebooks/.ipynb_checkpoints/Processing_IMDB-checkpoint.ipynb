{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This article is still __under development__: I'm currently working on adding more description and more algorithms.\n",
    "\n",
    "We work with a dataset containing 50,000 movie reviews from IMDB, labeled by sentiment (positive/negative).  In addition, there are another 50,000 IMDB reviews provided without any rating labels.  \n",
    "\n",
    "The reviews are split evenly into train and test sets (25k train and 25k test). The overall distribution of labels is also balanced within the train and test sets (12.5k pos and 12.5k neg).  Our goal is to predict sentiment in the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexis/anaconda/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os                                # accessing directory of files\n",
    "import pandas as pd                      # storing the data\n",
    "from bs4 import BeautifulSoup            # removing HTML tags\n",
    "import re                                # text processing with regular expressions\n",
    "from gensim.models import word2vec       # embedding algorithm\n",
    "import numpy as np                       # arrays and other mathy structures     \n",
    "from tqdm import tqdm                    # timing algorithms\n",
    "from gensim import models                # doc2vec implementation\n",
    "from random import shuffle               # for shuffling reviews\n",
    "from sklearn.linear_model import LogisticRegression  # LR implementation\n",
    "import nltk.data                         # sentence splitting\n",
    "from keras.models import Sequential, load_model      # MLP, RNN implementation\n",
    "from keras.layers import Dense, Dropout, LSTM        # MLP, RNN implementation\n",
    "from classes import Doc2VecUtility       # training, loading Doc2Vec embeddings for LR, MLP\n",
    "from classes import Word2VecUtility      # training, loading Word2Vec embeddings\n",
    "from classes import Word2VecRNNUtility   # training, loading Word2Vec embeddings for RNN\n",
    "import matplotlib.pyplot as plt          # plotting\n",
    "import plotly.plotly as py               # pretty plotting\n",
    "import plotly.graph_objs as go           # pretty plotting\n",
    "import seaborn as sns                    # pretty plotting\n",
    "import warnings                          # suppress (plotly) warnings\n",
    "import statistics                        # calculating median\n",
    "from keras.layers.embeddings import Embedding    # RNN implementation\n",
    "from keras.preprocessing import sequence # pre-process RNN data\n",
    "%matplotlib inline                       \n",
    "\n",
    "# If you are using Python 3, you will get an error.\n",
    "# (Pattern is a Python 2 library and fails to install for Python 3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be downloaded [here](http://ai.stanford.edu/~amaas/data/sentiment/).  We have written code to extract the reviews into Pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 12500 pos train reviews \n",
      " 12500 neg train reviews \n",
      " 12500 pos test reviews \n",
      " 12500 neg test reviews \n",
      " 50000 unsup reviews\n",
      "\n",
      " TOTAL: 100000 reviews\n"
     ]
    }
   ],
   "source": [
    "def load_data(directory_name):\n",
    "    # load dataset from directory to Pandas dataframe\n",
    "    data = []\n",
    "    files = [f for f in os.listdir('../../../aclImdb/' + directory_name)]\n",
    "    for f in files:\n",
    "        with open('../../../aclImdb/' + directory_name + f, \"r\", encoding = 'utf-8') as myfile:\n",
    "            data.append(myfile.read())\n",
    "    df = pd.DataFrame({'review': data, 'file': files})\n",
    "    return df\n",
    "\n",
    "# load training dataset\n",
    "train_pos = load_data('train/pos/')\n",
    "train_neg = load_data('train/neg/')\n",
    "\n",
    "# load test dataset\n",
    "test_pos = load_data('test/pos/')\n",
    "test_neg = load_data('test/neg/')\n",
    "\n",
    "# load unsupervised dataset\n",
    "unsup = load_data('train/unsup/')\n",
    "\n",
    "print(\"\\n %d pos train reviews \\n %d neg train reviews \\n %d pos test reviews \\n %d neg test reviews \\n %d unsup reviews\" \\\n",
    "      % (train_pos.shape[0], train_neg.shape[0], test_pos.shape[0], test_neg.shape[0], unsup.shape[0]))\n",
    "print(\"\\n TOTAL: %d reviews\" % int(train_pos.shape[0] + train_neg.shape[0] + test_pos.shape[0] + test_neg.shape[0] + unsup.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_pos`, `train_neg`, `test_pos`, `test_neg`, and `unsup` are Pandas dataframes.  They each have two columns, and each row corresponds to a review:\n",
    "- `file` : name of file that contains review\n",
    "- `review` : the full text of the review\n",
    "\n",
    "Each review is cleaned before being fed into a classification algorithm.\n",
    "- HTML tags are removed through the use of the Beautiful Soup library.\n",
    "- Punctuation is made consistent through the use of regular expressions.  This code appears below, in the `clean_str` method.\n",
    "- All words are converted to lowercase.\n",
    "- Each review is converted into either a list of words or to a list of lists of words.  In the latter, each list of lists corresponds to a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_str( string ):\n",
    "    # Function that cleans text using regular expressions\n",
    "    string = re.sub(r' +', ' ', string)\n",
    "    string = re.sub(r'\\.+', '.', string)\n",
    "    string = re.sub(r'\\.(?! )', '. ', string)    \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'m\", \" \\'m\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" ( \", string) \n",
    "    string = re.sub(r\"\\)\", \" ) \", string) \n",
    "    string = re.sub(r\"\\?\", \" ? \", string) \n",
    "    string = re.sub(r\"\\.\", \" . \", string)\n",
    "    string = re.sub(r\"\\-\", \" - \", string)\n",
    "    string = re.sub(r\"\\;\", \" ; \", string)\n",
    "    string = re.sub(r\"\\:\", \" : \", string)\n",
    "    string = re.sub(r'\\\"', ' \" ', string)\n",
    "    string = re.sub(r'\\/', ' / ', string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that there is still some room for improvement.  For instance, \n",
    "- Strings like \"Sgt. Cutter\" currently are broken into two sentences.  We should instead determine how to differentiate between periods that signify the end of an abbreviation and periods that denote the end of a sentence.\n",
    "- Some writers separate their sentences with commas or line breaks; the algorithm currently absorbs these multiple sentences into an individual sentence.\n",
    "- Ellipses (...) are currently processed as multiple, empty sentences (which are then discarded).\n",
    "\n",
    "Before writing this post, I read the Kaggle tutorial [here](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors).  My current processing algorithm borrows from that page, but also adds some meaningful improvements, partially informed by the algorithm [here](https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py).  For instance, \n",
    "- We keep the punctuation that ends each sentence (i.e., period vs. exclamation point), whereas punctuation was discarded in the Kaggle tutorial.  \n",
    "- We do smarter processing of contractions, in a way that understands that \"should've\" = \"should\" + \"'ve\".  In the Kaggle tutorial, \"should've\" is kept as a single word (contractions are not understood in terms of their composite parts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec + (LR, MLP) Implementation\n",
    "\n",
    "We can now see how well our Doc2Vec embedding performs.  To peek at the code under the hood, please look at the Python file [here](https://github.com/alexisbcook/Pelican_blog_all/blob/master/content/notebooks/classes/Doc2VecUtility.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[d2v_train, d2v_test] = Doc2VecUtility.get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = np.append(np.ones(12500), np.zeros(12500))\n",
    "test_labels = np.append(np.ones(12500), np.zeros(12500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89456000000000002"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(d2v_train, train_labels)\n",
    "\n",
    "# calculate accuracy\n",
    "classifier.score(d2v_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo!  We can predict sentiment with nearly 90 percent accuracy!  Can we do better?  Let's try out a MLP with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.2981 - acc: 0.8782     \n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.2626 - acc: 0.8920     \n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.2509 - acc: 0.8964     \n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.2402 - acc: 0.9020     \n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.2277 - acc: 0.9101     \n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.2138 - acc: 0.9179     \n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.2000 - acc: 0.9255     \n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.1838 - acc: 0.9319     \n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.1688 - acc: 0.9385     \n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.1567 - acc: 0.9431     \n",
      "\n",
      " 0.90496\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Dense(200, input_dim=300, init='uniform', activation='relu'))\n",
    "keras_model.add(Dropout(0.5))\n",
    "keras_model.add(Dense(1, activation='sigmoid'))\n",
    "keras_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "keras_model.fit(d2v_train, train_labels, nb_epoch=10, batch_size=20)\n",
    "\n",
    "# calculate accuracy\n",
    "loss, accuracy = keras_model.evaluate(d2v_test, test_labels, verbose=0)\n",
    "print(\"\\n\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, it's a slight improvement, but we haven't done much better.  To improve, our intuition tells us we should try some combination of:\n",
    "- better string cleaning,\n",
    "- testing other parameters for Doc2Vec embedding step, and\n",
    "- constructing deeper neural networks.\n",
    "\n",
    "However, it seems that Kaggle competitiors were not able to make this work -- that is, we won't be able to do much better with the current plan.  Thus, we will try something else, informed by the techniques [here](https://www.kaggle.com/c/word2vec-nlp-tutorial/forums/t/14966/post-competition-solutions).  Namely, we will train a few more embedding models and then use an ensemble method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec + (LR, MLP) Implementation\n",
    "\n",
    "We train a Word2Vec embedding, which we expect will get slightly worse performance than the Doc2Vec model.  To peek at the code under the hood, please look at the Python file [here](https://github.com/alexisbcook/Pelican_blog_all/blob/master/content/notebooks/classes/Word2VecUtility.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:12<00:00, 990.45it/s] \n",
      "100%|██████████| 12500/12500 [00:14<00:00, 886.47it/s]\n",
      "100%|██████████| 12500/12500 [00:13<00:00, 914.76it/s]\n",
      "100%|██████████| 12500/12500 [00:13<00:00, 912.28it/s]\n"
     ]
    }
   ],
   "source": [
    "[w2v_train, w2v_test] = Word2VecUtility.get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81396000000000002"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(w2v_train, train_labels)\n",
    "\n",
    "# calculate accuracy\n",
    "classifier.score(w2v_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.5198 - acc: 0.7566     \n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.3840 - acc: 0.8346     \n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.3590 - acc: 0.8455     \n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.3468 - acc: 0.8515     \n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.3389 - acc: 0.8562     \n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.3360 - acc: 0.8573     \n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.3321 - acc: 0.8610     \n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.3291 - acc: 0.8600     \n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.3260 - acc: 0.8619     \n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.3234 - acc: 0.8649     \n",
      "\n",
      " 0.86556\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Dense(200, input_dim=300, init='uniform', activation='relu'))\n",
    "keras_model.add(Dropout(0.5))\n",
    "keras_model.add(Dense(1, activation='sigmoid'))\n",
    "keras_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "keras_model.fit(w2v_train, train_labels, nb_epoch=10, batch_size=20)\n",
    "\n",
    "# calculate accuracy\n",
    "loss, accuracy = keras_model.evaluate(w2v_test, test_labels, verbose=0)\n",
    "print(\"\\n\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {Doc2Vec, Word2Vec} + (LR, MLP) Implementation\n",
    "\n",
    "As expected, we the Doc2Vec embedding provides better accuracy than the Word2Vec embedding (both with logistic regression and an MLP).  However, combining the two should result in an increase in performance, as we will see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine Doc2Vec and Word2Vec features\n",
    "d2v_w2v_train = np.append(d2v_train, w2v_train, axis=1)\n",
    "d2v_w2v_test = np.append(d2v_test, w2v_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89576"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(d2v_w2v_train, train_labels)\n",
    "\n",
    "# calculate accuracy\n",
    "classifier.score(d2v_w2v_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.2949 - acc: 0.8796     \n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.2594 - acc: 0.8926     \n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.2457 - acc: 0.9000     \n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.2339 - acc: 0.9038     \n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.2214 - acc: 0.9135     \n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.2059 - acc: 0.9207     \n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 4s - loss: 0.1913 - acc: 0.9278     \n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 4s - loss: 0.1781 - acc: 0.9344     \n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.1631 - acc: 0.9414     \n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.1502 - acc: 0.9470     \n",
      "\n",
      " 0.90576\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Dense(200, input_dim=600, init='uniform', activation='relu'))\n",
    "keras_model.add(Dropout(0.5))\n",
    "keras_model.add(Dense(1, activation='sigmoid'))\n",
    "keras_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "keras_model.fit(d2v_w2v_train, train_labels, nb_epoch=10, batch_size=20)\n",
    "\n",
    "# calculate accuracy\n",
    "loss, accuracy = keras_model.evaluate(d2v_w2v_test, test_labels, verbose=0)\n",
    "print(\"\\n\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, unfortunately, the performace is basically the same as when we used Doc2Vec alone.  \n",
    "\n",
    "# Word2Vec + RNN/LSTM Implementation\n",
    "\n",
    "To peek at the code under the hood, please look at the Python file [here](https://github.com/alexisbcook/Pelican_blog_all/blob/master/content/notebooks/classes/Word2VecRNNUtility.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:07<00:00, 1712.08it/s]\n",
      "100%|██████████| 12500/12500 [00:07<00:00, 1648.58it/s]\n",
      "100%|██████████| 12500/12500 [00:06<00:00, 1792.91it/s]\n",
      "100%|██████████| 12500/12500 [00:07<00:00, 1717.65it/s]\n"
     ]
    }
   ],
   "source": [
    "[w2vRNN_train, w2vRNN_test, embedding_weights] = Word2VecRNNUtility.get_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input has variable length!  Keras can't handle this ... so, let's figure out how to deal with it ...\n",
    "\n",
    "First, we calculate some statistics to determine how much information is lost when our reviews are truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum review length: 6 \n",
      "\n",
      "Maximum review length: 2582 \n",
      "\n",
      "Mean review length: 262.14742 \n",
      "\n",
      "Median review length: 197.0 \n",
      "\n",
      "Reviews of length >1000: 555 ( 1.11 % ),  <=1000: 49445 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~alexisbcook/54.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_lengths = [len(element) for element in w2vRNN_train + w2vRNN_test]\n",
    "print(\"Minimum review length:\", min(review_lengths), \"\\n\")\n",
    "print(\"Maximum review length:\", max(review_lengths), \"\\n\")\n",
    "print(\"Mean review length:\", np.mean(review_lengths), \"\\n\")\n",
    "print(\"Median review length:\", statistics.median(review_lengths), \"\\n\")\n",
    "# get number of reviews with more than 1000 words\n",
    "print(\"Reviews of length >1000:\", sum([i>1000 for i in review_lengths]), \n",
    "      \"(\", 100*sum([i>1000 for i in review_lengths])/len(review_lengths), \"% ), \",\n",
    "      \"<=1000:\", sum([i<=1000 for i in review_lengths]), \"\\n\")\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(title='Review Length'),\n",
    "    yaxis=dict(title='Count'),\n",
    "    bargap=0.1,\n",
    "    bargroupgap=0.1\n",
    ")\n",
    "\n",
    "data = [go.Histogram(\n",
    "    x=review_lengths,\n",
    "    autobinx=True,\n",
    "    opacity=0.75\n",
    ")]\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train only briefly, since RNNs are computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 1891s - loss: 0.4895 - acc: 0.7801  \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 1897s - loss: 0.3096 - acc: 0.8839  \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 5299s - loss: 0.2148 - acc: 0.9248  \n",
      "25000/25000 [==============================] - 161s   \n",
      "\n",
      " 0.86468\n"
     ]
    }
   ],
   "source": [
    "# process data \n",
    "w2vRNN_train_final = sequence.pad_sequences(w2vRNN_train, maxlen=600, dtype='int32', \\\n",
    "                                            padding='post', truncating='post', value=0.)\n",
    "w2vRNN_test_final = sequence.pad_sequences(w2vRNN_test, maxlen=600, dtype='int32', \\\n",
    "                                            padding='post', truncating='post', value=0.)\n",
    "\n",
    "# get the model\n",
    "if not os.path.isfile('models/rnn_model.h5'):\n",
    "    keras_model = Sequential()\n",
    "    keras_model.add(Embedding(output_dim=300, input_dim=len(embedding_weights), mask_zero=True, weights=[embedding_weights]))\n",
    "    keras_model.add(LSTM(10, return_sequences=False))  \n",
    "    keras_model.add(Dense(1, activation='sigmoid'))\n",
    "    keras_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    keras_model.fit(w2vRNN_train_final, train_labels, nb_epoch=3, batch_size=20)\n",
    "    # save the model\n",
    "    keras_model.save('models/rnn_model.h5')\n",
    "keras_model = load_model('models/rnn_model.h5')\n",
    "\n",
    "# calculate accuracy\n",
    "loss, accuracy = keras_model.evaluate(w2vRNN_test_final, test_labels, verbose=1)\n",
    "print(\"\\n\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Keras documentation [here](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py):\n",
    "\n",
    "> The IMDB dataset is too small for LSTM to be of any advantage compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "\n",
    "> RNNs are tricky. The coice of batch size, loss function, and optimizer are critical. Some configurations won't converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF + (LR, MLP) Implementation\n",
    "\n",
    "Coming soon!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
