{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STILL UNDER DEVELOPMENT (needs more description and different algorithms :P)\n",
    "\n",
    "We work with a dataset containing 50,000 movie reviews from IMDB, labeled by sentiment (positive/negative).  In addition, there are another 50,000 IMDB reviews provided without any rating labels.  \n",
    "\n",
    "The reviews are split evenly into train and test sets (25k train and 25k test). The overall distribution of labels is also balanced within the train and test sets (12.5k pos and 12.5k neg).  Our goal is to predict sentiment in the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexis/anaconda/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os                                # accessing directory of files\n",
    "import pandas as pd                      # storing the data\n",
    "from bs4 import BeautifulSoup            # removing HTML tags\n",
    "import re                                # text processing with regular expressions\n",
    "from gensim.models import word2vec       # embedding algorithm\n",
    "import numpy as np                       # arrays and other mathy structures     \n",
    "from tqdm import tqdm                    # timing algorithms\n",
    "from gensim import models                # doc2vec implementation\n",
    "from random import shuffle               # for shuffling reviews\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk.data                         # sentence splitting\n",
    "from keras.models import Sequential      # deep learning (part 1)\n",
    "from keras.layers import Dense, Dropout  # deep learning (part 2)\n",
    "from classes import Doc2VecUtility\n",
    "from classes import Word2VecUtility\n",
    "%matplotlib inline                       \n",
    "\n",
    "# If you are using Python 3, you will get an error.\n",
    "# (Pattern is a Python 2 library and fails to install for Python 3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be downloaded [here](http://ai.stanford.edu/~amaas/data/sentiment/).  We have written code to extract the reviews into Pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 12500 pos train reviews \n",
      " 12500 neg train reviews \n",
      " 12500 pos test reviews \n",
      " 12500 neg test reviews \n",
      " 50000 unsup reviews\n",
      "\n",
      " TOTAL: 100000 reviews\n"
     ]
    }
   ],
   "source": [
    "def load_data(directory_name):\n",
    "    # load dataset from directory to Pandas dataframe\n",
    "    data = []\n",
    "    files = [f for f in os.listdir('../../../aclImdb/' + directory_name)]\n",
    "    for f in files:\n",
    "        with open('../../../aclImdb/' + directory_name + f, \"r\", encoding = 'utf-8') as myfile:\n",
    "            data.append(myfile.read())\n",
    "    df = pd.DataFrame({'review': data, 'file': files})\n",
    "    return df\n",
    "\n",
    "# load training dataset\n",
    "train_pos = load_data('train/pos/')\n",
    "train_neg = load_data('train/neg/')\n",
    "\n",
    "# load test dataset\n",
    "test_pos = load_data('test/pos/')\n",
    "test_neg = load_data('test/neg/')\n",
    "\n",
    "# load unsupervised dataset\n",
    "unsup = load_data('train/unsup/')\n",
    "\n",
    "print(\"\\n %d pos train reviews \\n %d neg train reviews \\n %d pos test reviews \\n %d neg test reviews \\n %d unsup reviews\" \\\n",
    "      % (train_pos.shape[0], train_neg.shape[0], test_pos.shape[0], test_neg.shape[0], unsup.shape[0]))\n",
    "print(\"\\n TOTAL: %d reviews\" % int(train_pos.shape[0] + train_neg.shape[0] + test_pos.shape[0] + test_neg.shape[0] + unsup.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_pos`, `train_neg`, `test_pos`, `test_neg`, and `unsup` are Pandas dataframes.  They each have two columns, and each row corresponds to a review:\n",
    "- `file` : name of file that contains review\n",
    "- `review` : the full text of the review\n",
    "\n",
    "Each review is cleaned before being fed into a classification algorithm.\n",
    "- HTML tags are removed through the use of the Beautiful Soup library.\n",
    "- Punctuation is made consistent through the use of regular expressions.  This code appears below, in the `clean_str` method.\n",
    "- All words are converted to lowercase.\n",
    "- Each review is converted into either a list of words or to a list of lists of words.  In the latter, each list of lists corresponds to a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_str( string ):\n",
    "    # Function that cleans text using regular expressions\n",
    "    string = re.sub(r' +', ' ', string)\n",
    "    string = re.sub(r'\\.+', '.', string)\n",
    "    string = re.sub(r'\\.(?! )', '. ', string)    \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'m\", \" \\'m\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" ( \", string) \n",
    "    string = re.sub(r\"\\)\", \" ) \", string) \n",
    "    string = re.sub(r\"\\?\", \" ? \", string) \n",
    "    string = re.sub(r\"\\.\", \" . \", string)\n",
    "    string = re.sub(r\"\\-\", \" - \", string)\n",
    "    string = re.sub(r\"\\;\", \" ; \", string)\n",
    "    string = re.sub(r\"\\:\", \" : \", string)\n",
    "    string = re.sub(r'\\\"', ' \" ', string)\n",
    "    string = re.sub(r'\\/', ' / ', string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that there is still some room for improvement.  For instance, \n",
    "- Strings like \"Sgt. Cutter\" currently are broken into two sentences.  We should instead determine how to differentiate between periods that signify the end of an abbreviation and periods that denote the end of a sentence.\n",
    "- Some writers separate their sentences with commas or line breaks; the algorithm currently absorbs these multiple sentences into an individual sentence.\n",
    "- Ellipses (...) are currently processed as multiple, empty sentences (which are then discarded).\n",
    "\n",
    "Before writing this post, I read the Kaggle tutorial [here](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors).  My current processing algorithm borrows from that page, but also adds some meaningful improvements, partially informed by the algorithm [here](https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py).  For instance, \n",
    "- We keep the punctuation that ends each sentence (i.e., period vs. exclamation point), whereas punctuation was discarded in the Kaggle tutorial.  \n",
    "- We do smarter processing of contractions, in a way that understands that \"should've\" = \"should\" + \"'ve\".  In the Kaggle tutorial, \"should've\" is kept as a single word (contractions are not understood in terms of their composite parts).\n",
    "\n",
    "We can now see how well our Doc2Vec embedding performs.  To peek at the code under the hood, please look at the Python file [here](https://github.com/alexisbcook/Pelican_blog_all/blob/master/content/notebooks/classes/Doc2VecUtility.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[d2v_train, d2v_test] = Doc2VecUtility.get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = np.append(np.ones(12500), np.zeros(12500))\n",
    "test_labels = np.append(np.ones(12500), np.zeros(12500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89456000000000002"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(d2v_train, train_labels)\n",
    "classifier.score(d2v_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo!  We can predict sentiment with nearly 90 percent accuracy!  Can we do better?  Let's try out a MLP with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.2987 - acc: 0.8782     \n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.2624 - acc: 0.8904     \n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.2501 - acc: 0.8974     \n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.2370 - acc: 0.9046     \n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.2255 - acc: 0.9110     \n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.2130 - acc: 0.9187     \n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.1971 - acc: 0.9258     \n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.1843 - acc: 0.9316     \n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.1703 - acc: 0.9391     \n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.1578 - acc: 0.9437     \n",
      "\n",
      " 0.9056\n"
     ]
    }
   ],
   "source": [
    "# Now, we try a multilayer perceptron (with one hidden layer) in Keras !\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Dense(200, input_dim=300, init='uniform', activation='relu'))\n",
    "keras_model.add(Dropout(0.5))\n",
    "keras_model.add(Dense(1, activation='sigmoid'))\n",
    "keras_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "keras_model.fit(d2v_train, train_labels, nb_epoch=10, batch_size=20)\n",
    "loss, accuracy = keras_model.evaluate(d2v_test, test_labels, verbose=0)\n",
    "print(\"\\n\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, it's a slight improvement, but we haven't done much better.  To improve, our intuition tells us we should try some combination of:\n",
    "- better string cleaning,\n",
    "- testing other parameters for Doc2Vec embedding step, and\n",
    "- constructing deeper neural networks.\n",
    "\n",
    "However, it seems that Kaggle competitiors were not able to make this work -- that is, we won't be able to do much better with the current plan.  Thus, we will try something else, informed by the techniques [here](https://www.kaggle.com/c/word2vec-nlp-tutorial/forums/t/14966/post-competition-solutions).  Namely, we will train a few more embedding models and then use an ensemble method.\n",
    "\n",
    "We train a Word2Vec embedding, which we expect will get slightly worse performance than the Doc2Vec model.  To peek at the code under the hood, please look at the Python file [here](https://github.com/alexisbcook/Pelican_blog_all/blob/master/content/notebooks/classes/Word2VecUtility.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:12<00:00, 1028.26it/s]\n",
      "100%|██████████| 12500/12500 [00:12<00:00, 961.70it/s]\n",
      "100%|██████████| 12500/12500 [00:14<00:00, 892.04it/s]\n",
      "100%|██████████| 12500/12500 [00:13<00:00, 939.78it/s]\n"
     ]
    }
   ],
   "source": [
    "[w2v_train, w2v_test] = Word2VecUtility.get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81396000000000002"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(w2v_train, train_labels)\n",
    "classifier.score(w2v_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.5246 - acc: 0.7510     \n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.3867 - acc: 0.8326     \n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.3584 - acc: 0.8472     \n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.3470 - acc: 0.8516     \n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.3395 - acc: 0.8562     \n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.3358 - acc: 0.8568     \n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.3318 - acc: 0.8595     \n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.3298 - acc: 0.8615     \n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.3282 - acc: 0.8622     \n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 2s - loss: 0.3250 - acc: 0.8620     \n",
      "\n",
      " 0.86376\n"
     ]
    }
   ],
   "source": [
    "# Now, we try a multilayer perceptron (with one hidden layer) in Keras !\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Dense(200, input_dim=300, init='uniform', activation='relu'))\n",
    "keras_model.add(Dropout(0.5))\n",
    "keras_model.add(Dense(1, activation='sigmoid'))\n",
    "keras_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "keras_model.fit(w2v_train, train_labels, nb_epoch=10, batch_size=20)\n",
    "loss, accuracy = keras_model.evaluate(w2v_test, test_labels, verbose=0)\n",
    "print(\"\\n\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we the Doc2Vec embedding provides better accuracy than the Word2Vec embedding (both with logistic regression and an MLP).  However, combining the two should result in an increase in performance, as we will see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine Doc2Vec and Word2Vec features\n",
    "d2v_w2v_train = np.append(d2v_train, w2v_train, axis=1)\n",
    "d2v_w2v_test = np.append(d2v_test, w2v_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89576"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(d2v_w2v_train, train_labels)\n",
    "classifier.score(d2v_w2v_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.2955 - acc: 0.8801     \n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.2602 - acc: 0.8928     \n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.2465 - acc: 0.8998     \n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.2359 - acc: 0.9051     \n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.2227 - acc: 0.9132     \n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.2104 - acc: 0.9197     \n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.1957 - acc: 0.9257     \n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.1817 - acc: 0.9340     \n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.1650 - acc: 0.9414     \n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 3s - loss: 0.1531 - acc: 0.9469     \n",
      "\n",
      " 0.905\n"
     ]
    }
   ],
   "source": [
    "# Now, we try a multilayer perceptron (with one hidden layer) in Keras !\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Dense(200, input_dim=600, init='uniform', activation='relu'))\n",
    "keras_model.add(Dropout(0.5))\n",
    "keras_model.add(Dense(1, activation='sigmoid'))\n",
    "keras_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "keras_model.fit(d2v_w2v_train, train_labels, nb_epoch=10, batch_size=20)\n",
    "loss, accuracy = keras_model.evaluate(d2v_w2v_test, test_labels, verbose=0)\n",
    "print(\"\\n\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the performace is basically the same as when we used Doc2Vec alone.  I bet we can do better :)!  More on that soon!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
