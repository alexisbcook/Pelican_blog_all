{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We work with a dataset containing 50,000 movie reviews from IMDB, labeled by sentiment (positive/negative).  In addition, there are another 50,000 IMDB reviews provided without any rating labels.  \n",
    "\n",
    "The reviews are split evenly into train and test sets (25k train and 25k test). The overall distribution of labels is also balanced within the train and test sets (12.5k pos and 12.5k neg).  Our goal is to predict sentiment in the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexis/anaconda/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os                                # accessing directory of files\n",
    "import pandas as pd                      # storing the data\n",
    "from bs4 import BeautifulSoup            # removing HTML tags\n",
    "import re                                # text processing with regular expressions\n",
    "from gensim.models import word2vec       # embedding algorithm\n",
    "import numpy as np                       # arrays and other mathy structures     \n",
    "from tqdm import tqdm                    # timing algorithms\n",
    "from gensim import models                # doc2vec implementation\n",
    "from random import shuffle               # for shuffling reviews\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk.data                         # sentence splitting\n",
    "from keras.models import Sequential      # deep learning (part 1)\n",
    "from keras.layers import Dense, Dropout  # deep learning (part 2)\n",
    "%matplotlib inline                       \n",
    "\n",
    "# If you are using Python 3, you will get an error.\n",
    "# (Pattern is a Python 2 library and fails to install for Python 3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be downloaded [here](http://ai.stanford.edu/~amaas/data/sentiment/).  We first write code to extract the reviews into Pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 12500 pos train reviews \n",
      " 12500 neg train reviews \n",
      " 12500 pos test reviews \n",
      " 12500 neg test reviews \n",
      " 50000 unsup reviews\n",
      "\n",
      " TOTAL: 100000 reviews\n"
     ]
    }
   ],
   "source": [
    "def load_data(directory_name):\n",
    "    # load dataset from directory to Pandas dataframe\n",
    "    data = []\n",
    "    files = [f for f in os.listdir('../../../aclImdb/' + directory_name)]\n",
    "    for f in files:\n",
    "        with open('../../../aclImdb/' + directory_name + f, \"r\", encoding = 'utf-8') as myfile:\n",
    "            data.append(myfile.read())\n",
    "    df = pd.DataFrame({'review': data, 'file': files})\n",
    "    return df\n",
    "\n",
    "# load training dataset\n",
    "train_pos = load_data('train/pos/')\n",
    "train_neg = load_data('train/neg/')\n",
    "\n",
    "# load test dataset\n",
    "test_pos = load_data('test/pos/')\n",
    "test_neg = load_data('test/neg/')\n",
    "\n",
    "# load unsupervised dataset\n",
    "unsup = load_data('train/unsup/')\n",
    "\n",
    "print(\"\\n %d pos train reviews \\n %d neg train reviews \\n %d pos test reviews \\n %d neg test reviews \\n %d unsup reviews\" \\\n",
    "      % (train_pos.shape[0], train_neg.shape[0], test_pos.shape[0], test_neg.shape[0], unsup.shape[0]))\n",
    "print(\"\\n TOTAL: %d reviews\" % int(train_pos.shape[0] + train_neg.shape[0] + test_pos.shape[0] + test_neg.shape[0] + unsup.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_pos`, `train_neg`, `test_pos`, `test_neg`, and `unsup` are Pandas dataframes.  They each have two columns, and each row corresponds to a review:\n",
    "- `file` : name of file that contains review\n",
    "- `review` : the full text of the review\n",
    "\n",
    "We write a function `review_to_wordlist`, which processes each review as follows:\n",
    "- Punctuation is made consistent through the use of regular expressions.\n",
    "- HTML tags are removed through the use of the Beautiful Soup library.\n",
    "- All words are converted to lowercase.\n",
    "- Each review is converted into a list of words.\n",
    "\n",
    "We note that there is still some room for improvement.  For instance, \n",
    "- Strings like \"Sgt. Cutter\" currently are broken into two sentences.  We should instead determine how to differentiate between periods that signify the end of an abbreviation and periods that denote the end of a sentence.\n",
    "- Some writers separate their sentences with commas or line breaks; the algorithm currently absorbs these multiple sentences into an individual sentence.\n",
    "- Ellipses (...) are currently processed as multiple, empty sentences (which are then discarded).\n",
    "\n",
    "Before writing this post, I read the Kaggle tutorial [here](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors).  My current processing algorithm borrows from that page, but also adds some meaningful improvements, partially informed by the algorithm [here](https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py).  For instance, \n",
    "- We keep the punctuation that ends each sentence (i.e., period vs. exclamation point), whereas punctuation was discarded in the Kaggle tutorial.  \n",
    "- We do smarter processing of contractions, in a way that understands that \"should've\" = \"should\" + \"'ve\".  In the Kaggle tutorial, \"should've\" is kept as a single word (contractions are not understood in terms of their composite parts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_str( string ):\n",
    "    # Function that cleans text using regular expressions\n",
    "    string = re.sub(r' +', ' ', string)\n",
    "    string = re.sub(r'\\.+', '.', string)\n",
    "    string = re.sub(r'\\.(?! )', '. ', string)    \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'m\", \" \\'m\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" ( \", string) \n",
    "    string = re.sub(r\"\\)\", \" ) \", string) \n",
    "    string = re.sub(r\"\\?\", \" ? \", string) \n",
    "    string = re.sub(r\"\\.\", \" . \", string)\n",
    "    string = re.sub(r\"\\-\", \" - \", string)\n",
    "    string = re.sub(r\"\\;\", \" ; \", string)\n",
    "    string = re.sub(r\"\\:\", \" : \", string)\n",
    "    string = re.sub(r'\\\"', ' \" ', string)\n",
    "    string = re.sub(r'\\/', ' / ', string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each cleaned review is fed into the `LabeledLineReview` class, written below.  These labeled reviews are fed into the Doc2Vec algorithm to obtain an embedding of each review.  We note that we use the full set of 100,000 reviews to learn the embedding, but our classification algorithm will be trained with the training set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Doc2VecUtility(object):\n",
    "\n",
    "    def review_to_wordlist( review ):\n",
    "        #\n",
    "        # Function to turn each review into a list of sentences, \n",
    "        # where each sentence is a list of words\n",
    "        #\n",
    "        # 1. Process punctuation, excessive periods, missing spaces \n",
    "        review = clean_str(review)\n",
    "        #\n",
    "        # 2. Remove HTML tags \n",
    "        review = BeautifulSoup(review, \"lxml\").get_text()\n",
    "        #\n",
    "        # 3. remove white spaces\n",
    "        review = review.strip()\n",
    "        #\n",
    "        # 4. return lowercase collection of words\n",
    "        wordlist = review.lower().split()\n",
    "        #\n",
    "        # 5. Return the list of words\n",
    "        return wordlist\n",
    "\n",
    "    class LabeledLineReview(object):\n",
    "        def __init__(self, dflist):\n",
    "            self.dflist = dflist\n",
    "\n",
    "        def __iter__(self):\n",
    "            for df in self.dflist:\n",
    "                for idx in tqdm(df.index):\n",
    "                    yield models.doc2vec.LabeledSentence(review_to_wordlist(df.ix[idx, 'review']), [df.ix[idx, 'file']])\n",
    "\n",
    "        def to_array(self):\n",
    "            self.reviews = []\n",
    "            for df in self.dflist:\n",
    "                for idx in tqdm(df.index):\n",
    "                    self.reviews.append(models.doc2vec.LabeledSentence(review_to_wordlist(df.ix[idx, 'review']), [df.ix[idx, 'file']]))\n",
    "            return self.reviews\n",
    "\n",
    "        def reviews_perm(self):\n",
    "            shuffle(self.reviews)\n",
    "            return self.reviews\n",
    "    \n",
    "    def train(self):\n",
    "        #\n",
    "        # Trains the doc2vec model\n",
    "        #\n",
    "        # 1. Get all reviews together\n",
    "        reviews = LabeledLineReview([train_pos, train_neg, test_pos, test_neg, unsup])\n",
    "        \n",
    "        # 2. Set values for various parameters and define the model\n",
    "        num_features = 100    # Word vector dimensionality                      \n",
    "        min_word_count = 1   # Minimum word count                        \n",
    "        num_workers = 8       # Number of threads to run in parallel\n",
    "        context = 10          # Context window size                                                                                    \n",
    "        downsampling = 1e-4   # Downsample setting for frequent words\n",
    "        #\n",
    "        model = models.Doc2Vec(workers = num_workers, \\\n",
    "                               size = num_features, min_count = min_word_count, \\\n",
    "                               window = context, sample = downsampling, negative = 5)\n",
    "        model.build_vocab(reviews.to_array())\n",
    "        #\n",
    "        # 3. Train the model\n",
    "        for epoch in tqdm(range(10)):\n",
    "            model.train(reviews.reviews_perm())\n",
    "        #\n",
    "        # 4. Save the model\n",
    "        model.init_sims(replace=True)\n",
    "        model.save(\"models/d2v\")\n",
    "                \n",
    "    def get_embedding(self):\n",
    "        #\n",
    "        # Returns embedding and labels, training model if necessary\n",
    "        #\n",
    "        # 1. Load the saved model.\n",
    "        #   (If the model is not already saved, train the model)\n",
    "        if not os.path.isfile('models/d2v'):\n",
    "            self.train()\n",
    "        model = models.Doc2Vec.load(\"models/d2v\")\n",
    "        #\n",
    "        # 2. Obtain train data embeddings and labels\n",
    "        train_array = np.zeros((25000, 100))\n",
    "        train_tags = list(train_pos['file'].values) + list(train_neg['file'].values)\n",
    "        for idx , val in enumerate(train_tags):\n",
    "            train_array[idx] = model.docvecs[val]\n",
    "        train_labels = np.append(np.ones(12500), np.zeros(12500))\n",
    "        #\n",
    "        # 3. Obtain test data embeddings and labels\n",
    "        test_array = np.zeros((25000, 100))\n",
    "        test_tags = list(test_pos['file'].values) + list(test_neg['file'].values)\n",
    "        for idx , val in enumerate(test_tags):\n",
    "            test_array[idx] = model.docvecs[val]\n",
    "        test_labels = np.append(np.ones(12500), np.zeros(12500))\n",
    "        #\n",
    "        # 4. Return embeddings and labels\n",
    "        return train_array, train_labels, test_array, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[d2v_train, train_labels, d2v_test, test_labels] = Doc2VecUtility().get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8992"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(d2v_train, train_labels)\n",
    "classifier.score(d2v_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo!  We can predict sentiment with nearly 90 percent accuracy!  Can we do better?  Let's try out a MLP with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2903 - acc: 0.8821     \n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2648 - acc: 0.8892     \n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2583 - acc: 0.8943     \n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2536 - acc: 0.8983     \n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2480 - acc: 0.8996     \n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2442 - acc: 0.9026     \n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2400 - acc: 0.9063     \n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2366 - acc: 0.9064     \n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2327 - acc: 0.9104     \n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2274 - acc: 0.9116     \n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2251 - acc: 0.9159     \n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2224 - acc: 0.9174     \n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2180 - acc: 0.9181     \n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2123 - acc: 0.9204     \n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2122 - acc: 0.9228     \n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2091 - acc: 0.9228     \n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2035 - acc: 0.9238     \n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2012 - acc: 0.9267     \n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.1999 - acc: 0.9281     \n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.1956 - acc: 0.9294     \n",
      "\n",
      " 0.90508\n"
     ]
    }
   ],
   "source": [
    "# Now, we try a multilayer perceptron (with one hidden layer) in Keras !\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Dense(200, input_dim=100, init='uniform', activation='relu'))\n",
    "keras_model.add(Dropout(0.5))\n",
    "keras_model.add(Dense(1, activation='sigmoid'))\n",
    "keras_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "keras_model.fit(d2v_train, train_labels, nb_epoch=20, batch_size=20)\n",
    "loss, accuracy = keras_model.evaluate(d2v_test, test_labels, verbose=0)\n",
    "print(\"\\n\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, it's a slight improvement, but we haven't done much better.  To improve, our intuition tells us we should try some combination of:\n",
    "- better string cleaning,\n",
    "- testing other parameters for Doc2Vec embedding step, and\n",
    "- constructing deeper neural networks.\n",
    "\n",
    "However, it seems that Kaggle competitiors were not able to make this work -- that is, we won't be able to do much better with the current plan.  Thus, we will try something else, informed by the techniques [here](https://www.kaggle.com/c/word2vec-nlp-tutorial/forums/t/14966/post-competition-solutions).  Namely, we will train a few more models and then use an ensemble method.\n",
    "\n",
    "First, we train a Word2Vec embedding, which we expect will get slightly worse performance than the Doc2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Word2VecUtility(object):\n",
    "    \n",
    "    def getAvgFeatureVecs(df, model, num_features):\n",
    "        #\n",
    "        # Given a df of reviews, calculate the average feature vector for each one \n",
    "        # \n",
    "        index2word_set = set(model.index2word)\n",
    "        reviewFeatureVecs = []\n",
    "        for idx in tqdm(df.index):\n",
    "            to_append = makeAvgFeatureVec(df.ix[idx, 'review'], model, index2word_set, num_features)\n",
    "            reviewFeatureVecs.append(to_append)\n",
    "        reviewFeatureVecs = np.array(reviewFeatureVecs)\n",
    "        return reviewFeatureVecs\n",
    "\n",
    "    def makeAvgFeatureVec(review, model, index2word_set, num_features):\n",
    "        #\n",
    "        # Averages all of the word vectors in a given review\n",
    "        #\n",
    "        # 1. Pre-initialize an empty numpy array (for speed)\n",
    "        featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "        #\n",
    "        # 2. Initialize number of words in review\n",
    "        nwords = 0.\n",
    "        # \n",
    "        # 3. Loop over each word in the review \n",
    "        #    If it is in the model's vocab, add its feature vector to the total\n",
    "        words = review_to_wordlist(review)\n",
    "        for word in words:\n",
    "            if word in index2word_set: \n",
    "                nwords = nwords + 1.\n",
    "                featureVec = np.add(featureVec, model[word])\n",
    "        # \n",
    "        # 4. Divide the result by the number of words to get the average\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "        # 5. Return the average word vector\n",
    "        return featureVec\n",
    "    \n",
    "    def review_to_wordlist( review, only_words = False ):\n",
    "        #\n",
    "        # Function to convert a document to a sequence of words,\n",
    "        # optionally removing stop words.  Returns a list of words.\n",
    "        #\n",
    "        # 1. Remove HTML\n",
    "        review = BeautifulSoup(review, \"lxml\").get_text()\n",
    "        #\n",
    "        # 2. Process punctuation, excessive periods, missing spaces\n",
    "        review = clean_str(review)\n",
    "        #\n",
    "        # 3. (Optionally) remove non-letters / non-words\n",
    "        if only_words:\n",
    "            review = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
    "        #\n",
    "        # 4. Convert words to lower case and split into list\n",
    "        words = review.lower().split()\n",
    "        #\n",
    "        # 5. Return a list of words\n",
    "        return(words)\n",
    "    \n",
    "    def review_to_lists_of_lists( review, only_words = False ):\n",
    "        # \n",
    "        # Function to turn each review into a list of sentences, \n",
    "        # where each sentence is a list of words\n",
    "        #\n",
    "        # 1. Process punctuation, excessive periods, missing spaces \n",
    "        review = BeautifulSoup(review, \"lxml\").get_text()\n",
    "        #\n",
    "        # 2. Remove HTML tags \n",
    "        review = clean_str(review)\n",
    "        #\n",
    "        # 3. (Optionally) remove non-letters / non-words\n",
    "        if only_words:\n",
    "            review = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
    "        #\n",
    "        # 4. Use the NLTK tokenizer to split the review into list of sentences\n",
    "        #   (getting rid of extra spaces at front/back)\n",
    "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        raw_sentences = tokenizer.tokenize(review.strip())\n",
    "        #\n",
    "        # 5. Loop over each sentence to get list of list of lowercase words\n",
    "        sentences = []\n",
    "        for raw_sentence in raw_sentences:\n",
    "            # Convert to lowercase and split into list of words\n",
    "            raw_sentence = raw_sentence.lower().split()\n",
    "            # If a sentence is not long enough, skip it\n",
    "            if len(raw_sentence) > 1:\n",
    "                # add list of words to returned object\n",
    "                sentences.append( raw_sentence )\n",
    "        #\n",
    "        # 6. Return the list of sentences (each sentence is a list of words,\n",
    "        # so returns a list of lists)\n",
    "        return sentences\n",
    "\n",
    "    def corpus_to_list(df, tokenizer):\n",
    "        # Turns dataframe of reviews into a list of sentences,\n",
    "        # where each sentence is a list of words\n",
    "        # and sentences are derived from *all reviews* in dataframe df\n",
    "        sentences = []\n",
    "        for idx in tqdm(df.index):\n",
    "            to_append = review_to_lists_of_lists(df.ix[idx, 'review'])\n",
    "            sentences += to_append\n",
    "        return sentences\n",
    "\n",
    "    def train(self):\n",
    "        #\n",
    "        # Trains the word2vec model\n",
    "        #\n",
    "        # 1. Assemble all reviews\n",
    "        train_pos_sentences = corpus_to_list(train_pos, tokenizer)\n",
    "        train_neg_sentences = corpus_to_list(train_neg, tokenizer)\n",
    "        test_pos_sentences = corpus_to_list(test_pos, tokenizer)\n",
    "        test_neg_sentences = corpus_to_list(test_neg, tokenizer)\n",
    "        unsup_sentences = corpus_to_list(unsup, tokenizer)\n",
    "        sentences = train_pos_sentences + train_neg_sentences + test_pos_sentences + test_neg_sentences + unsup_sentences\n",
    "        #\n",
    "        # 2. Set values for various parameters \n",
    "        num_features = 300    # Word vector dimensionality                      \n",
    "        min_word_count = 40   # Minimum word count                        \n",
    "        num_workers = 4       # Number of threads to run in parallel\n",
    "        context = 10          # Context window size                                                                                    \n",
    "        downsampling = 1e-3   # Downsample setting for frequent words\n",
    "        #\n",
    "        # 3. Initialize and train the model \n",
    "        model = word2vec.Word2Vec(sentences, workers = num_workers, \\\n",
    "                    size = num_features, min_count = min_word_count, \\\n",
    "                    window = context, sample = downsampling)\n",
    "        #\n",
    "        # 4. Save the model\n",
    "        model.init_sims(replace=True)\n",
    "        model.save(\"models/w2v\")\n",
    "            \n",
    "    def get_embedding(self):\n",
    "        #\n",
    "        # Returns embedding and labels, training model if necessary\n",
    "        #\n",
    "        # 1. Load the saved model.\n",
    "        #   (If the model is not already saved, train the model)\n",
    "        if not os.path.isfile('models/w2v'):\n",
    "            self.train()\n",
    "        model = models.Doc2Vec.load(\"models/w2v\")\n",
    "        #\n",
    "        # 2. Obtain train data embeddings \n",
    "        pos_w2v_train = getAvgFeatureVecs(train_pos, model, 300)\n",
    "        neg_w2v_train = getAvgFeatureVecs(train_neg, model, 300)\n",
    "        w2v_train = np.append(pos_w2v_train, neg_w2v_train, axis=0)\n",
    "        #\n",
    "        # 3. Obtain test data embeddings\n",
    "        pos_w2v_test = getAvgFeatureVecs(test_pos, model, 300)\n",
    "        neg_w2v_test = getAvgFeatureVecs(test_neg, model, 300)\n",
    "        w2v_test = np.append(pos_w2v_test, neg_w2v_test, axis=0)\n",
    "        #\n",
    "        # 4. Return all embeddings\n",
    "        return w2v_train, w2v_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:13<00:00, 899.14it/s]\n",
      "100%|██████████| 12500/12500 [00:13<00:00, 903.10it/s]\n",
      "100%|██████████| 12500/12500 [00:13<00:00, 959.36it/s]\n",
      "100%|██████████| 12500/12500 [00:12<00:00, 993.89it/s]\n"
     ]
    }
   ],
   "source": [
    "w2v_train, w2v_test = Word2VecUtility().get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81228"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(w2v_train, train_labels)\n",
    "classifier.score(w2v_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see a drop in performance relative to the Doc2Vec embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.5282 - acc: 0.7515     \n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.3917 - acc: 0.8305     \n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.3612 - acc: 0.8446     \n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.3487 - acc: 0.8525     \n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.3423 - acc: 0.8563     \n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.3362 - acc: 0.8576     \n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.3327 - acc: 0.8604     \n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.3304 - acc: 0.8615     \n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.3278 - acc: 0.8629     \n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.3264 - acc: 0.8615     \n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.3239 - acc: 0.8656     \n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.3234 - acc: 0.8646     \n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.3223 - acc: 0.8655     \n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.3210 - acc: 0.8664     \n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.3195 - acc: 0.8676     \n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.3197 - acc: 0.8672     \n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.3181 - acc: 0.8677     \n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.3183 - acc: 0.8666     \n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.3164 - acc: 0.8696     \n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.3158 - acc: 0.8692     \n",
      "\n",
      " 0.86204\n"
     ]
    }
   ],
   "source": [
    "# Now, we try a multilayer perceptron (with one hidden layer) in Keras !\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Dense(200, input_dim=300, init='uniform', activation='relu'))\n",
    "keras_model.add(Dropout(0.5))\n",
    "keras_model.add(Dense(1, activation='sigmoid'))\n",
    "keras_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "keras_model.fit(w2v_train, train_labels, nb_epoch=20, batch_size=20)\n",
    "loss, accuracy = keras_model.evaluate(w2v_test, test_labels, verbose=0)\n",
    "print(\"\\n\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we also see a drop in accuracy in the neural network with the Word2Vec embedding.\n",
    "However, combining the two embeddings shoul result in an increase in performance, as we will see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine Doc2Vec and Word2Vec features\n",
    "train = np.append(d2v_train,w2v_train,axis=1)\n",
    "test = np.append(d2v_test,w2v_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90007999999999999"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(train, train_labels)\n",
    "classifier.score(test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a marginal increase in performance, relative to when we used Doc2Vec features alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2897 - acc: 0.8848     \n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2612 - acc: 0.8934     \n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2538 - acc: 0.8955     \n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2494 - acc: 0.9002     \n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2452 - acc: 0.9019     \n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2378 - acc: 0.9046     \n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 2s - loss: 0.2354 - acc: 0.9088     \n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.2306 - acc: 0.9100     \n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.2248 - acc: 0.9128     \n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.2203 - acc: 0.9158     \n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.2163 - acc: 0.9176     \n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.2117 - acc: 0.9200     \n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.2091 - acc: 0.9228     \n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.2045 - acc: 0.9235     \n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.2006 - acc: 0.9258     \n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 3s - loss: 0.1986 - acc: 0.9280     \n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 4s - loss: 0.1953 - acc: 0.9302     \n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 4s - loss: 0.1903 - acc: 0.9310     \n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 4s - loss: 0.1882 - acc: 0.9330     \n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 4s - loss: 0.1857 - acc: 0.9324     \n",
      "\n",
      " 0.9072\n"
     ]
    }
   ],
   "source": [
    "# Now, we try a multilayer perceptron (with one hidden layer) in Keras !\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Dense(200, input_dim=400, init='uniform', activation='relu'))\n",
    "keras_model.add(Dropout(0.5))\n",
    "keras_model.add(Dense(1, activation='sigmoid'))\n",
    "keras_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "keras_model.fit(train, train_labels, nb_epoch=20, batch_size=20)\n",
    "loss, accuracy = keras_model.evaluate(test, test_labels, verbose=0)\n",
    "print(\"\\n\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, only a slight increase in performance over Doc2Vec alone.  I bet we can do better :)!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
