{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at two approaches to face detection that both first find a lower-dimensional subspace onto which to project an image of a face.\n",
    "\n",
    "Recall that **covariance** is a measure of how two random variables change together.  It is a scalar value whose magnitude is not easily interpretable.  We say that two random variables are **uncorrelated** if and only if their covariance is zero. \n",
    "\n",
    "pictures of uncorrelated random variables, idea of choosing new basis functions ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and Eigenfaces\n",
    "\n",
    "Given a dataset $X$ containing $n$ data points, each with $m$ features, we would like to find a new set of features that captures the same relevant patterns in the data, but with an increased signal-to-noise ratio.  To accomplish this, we first find a well-informed change-of-basis (where the signal is concentrated primarily , and then drop noisy components.\n",
    "\n",
    "Suppose our dataset is contained in an $m \\times n $ matrix $X$, whose $j$-th column $x_{\\cdot j} \\in \\mathbb{R}^m$ is the $j$-th datapoint.  Note that the column space is at most an $m$-dimensional object, anyway, and so we need only $m$ vectors to span it.\n",
    "\n",
    "Let $P$ be an $m \\times m$ matrix, and let $p_i$ be the $i$-th column.  Suppose that the $\\{p_i\\}_{i=1}^m$ are a set of orthogonal vectors that span the same space as our dataset.  Then we can express each data point in terms of the new basis and write that for all $j \\in \\{1, \\ldots, n\\}$,\n",
    "\n",
    "$$\n",
    "x_{\\cdot j} = \\sum_{i=1}^m (p_i \\cdot x_{\\cdot j}) p_i.\n",
    "$$\n",
    "\n",
    "We can keep track of the coefficients in a matrix $Y = P^TX$, whose $i,j$-th entry $Y_{ij}$ is $p_i \\cdot x_{\\cdot j}$.  \n",
    "\n",
    "We are interested in choosing some $m'<m$ so that\n",
    "$$\n",
    "x_{\\cdot j} \\approx \\sum_{i=1}^{m'} (p_i \\cdot x_{\\cdot j}) p_i,\n",
    "$$\n",
    "\n",
    "where noise is gone but pattern still there.  How do we choose the entries of our matrix $P$?\n",
    "\n",
    "Well ... we want greatest variance on projection on first axis, second greatest on second, and so on.  in this way, can drop components.  arrive at equivalent change-of-basis when try to render set of variables as uncorrelated set.\n",
    "\n",
    "The covariance of the projected points can be calculated in covariance matrix as\n",
    "\n",
    "$$\n",
    "S_Y = \\frac{1}{n}YY^T = \\frac{1}{n}(P^TX)(P^TX)^T = \\frac{1}{n}P^T(XX^T)P = \\frac{1}{n}P^TAP,\n",
    "$$\n",
    "\n",
    "where we have set $A\\triangleq XX^T$.  Since $A$ is a symmetric matrix, it is orthogonally diagonalizable (let $A = \\Sigma D \\Sigma^T$), and so letting $P = \\Sigma$ yields $S_Y$ as a diagonal matrix.  Thus, we have our projection directions.\n",
    "\n",
    "Alternatively, can view as first eigenvector with most variance, and so on.  Then, the first projection direction (we'll see it corresponds to the principal eigenvector!) is yielded as:\n",
    "\n",
    "$$\n",
    "v_1 = \\arg\\max_{\\|v\\| = 1} \\sum_{i=1}^n y_i^2 = \\arg\\max_{\\|v\\| = 1} \\sum_{i=1}^n (v \\cdot x_i)^2 = \\arg\\max_{\\|v\\| = 1} \\|X v\\|^2 = \\arg\\max_{\\|v\\| = 1} v^TX^TXv = \\arg\\max \\frac{v^TX^TXv}{v^Tv}.\n",
    "$$\n",
    "\n",
    "The latter quantity can be recognized as a Rayleigh quotient, which is maximized when $v$ is the principal eigenvector of $X^TX$.  We can see that we can rank the eigenvectors, and the one with most variance (variance when proj onto that eigenvector is exactly the rayleigh quotient maximized, or the eigenvalue correspponding to the principal eigenvalue) is the principal.  Therefore, to project onto lower-dimensional (less than $m$) space, just need to pick eigenvectors corresponding to largest eigenvalues.  Drop the ones with smaller eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA and Fisherfaces\n",
    "\n",
    "LDA follows the same principle as PCA, but makes use of the labels that accompany each datapoint.  (PCA ignored them and in some sense can be thought of as \"unsupervised\"; LDA takes a \"supervised\" approach.)  The main idea is that we would like to find a projection where data points with the same label are close together, and separated from points with a different label. **linear discriminant analysis not to be confused with latent dirichlet allocation!**. characterizes points (still keep patterns) but also separates data points belonging to different classes.\n",
    "\n",
    "Towards that end, we define two metrics: one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
